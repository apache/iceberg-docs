<!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=author content><title>Procedures</title>
<link href=../css/bootstrap.css rel=stylesheet><link href=../css/markdown.css rel=stylesheet><link href=../css/katex.min.css rel=stylesheet><link href=../css/iceberg-theme.css rel=stylesheet><link href=../font-awesome-4.7.0/css/font-awesome.min.css rel=stylesheet type=text/css><link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel=stylesheet type=text/css><link href=../css/termynal.css rel=stylesheet></head><body><head><script>function addAnchor(e){e.insertAdjacentHTML("beforeend",`<a href="#${e.id}" class="anchortag" ariaLabel="Anchor"> 🔗 </a>`)}document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll("h1[id], h2[id], h3[id], h4[id]");e&&e.forEach(addAnchor)})</script></head><nav class="navbar navbar-default" role=navigation><topsection><div class=navbar-fixed-top><div><button type=button class=navbar-toggle data-toggle=collapse data-target=div.sidebar>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class="page-scroll navbar-brand" href=https://iceberg.apache.org/><img class=top-navbar-logo src=https://iceberg.apache.org/docs/fd-update-latestt//img/iceberg-logo-icon.png> Apache Iceberg</a></div><div><input type=search class=form-control id=search-input placeholder=Search... maxlength=64 data-hotkeys=s/></div><div class=versions-dropdown><span>1.4.2</span> <i class="fa fa-chevron-down"></i><div class=versions-dropdown-content><ul><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../latest>latest</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../1.4.2>1.4.2</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../1.4.1>1.4.1</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../1.4.0>1.4.0</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../1.3.1>1.3.1</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../1.3.0>1.3.0</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../1.2.1>1.2.1</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../1.2.0>1.2.0</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../1.1.0>1.1.0</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../1.0.0>1.0.0</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../0.14.1>0.14.1</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../0.14.0>0.14.0</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../0.13.2>0.13.2</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../0.13.1>0.13.1</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../0.13.0>0.13.0</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../0.12.1>0.12.1</a></li></ul></div></div></div><div class="navbar-menu-fixed-top navbar-pages-group"><div class=versions-dropdown><div class=topnav-page-selection><a href>Quickstart</a> <i class="fa fa-chevron-down"></i></div class="topnav-page-selection"><div class=versions-dropdown-content><ul><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../hive-quickstart>Hive</a></li class="topnav-page-selection"><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../spark-quickstart>Spark</a></li class="topnav-page-selection"></ul></div></div><div class=topnav-page-selection><a id=active href=https://iceberg.apache.org/docs/fd-update-latestt/../../docs/latest>Docs</a></div><div class=topnav-page-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../releases>Releases</a></div class="topnav-page-selection"><div class=topnav-page-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../roadmap>Roadmap</a></div class="topnav-page-selection"><div class=topnav-page-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../blogs>Blogs</a></div class="topnav-page-selection"><div class=topnav-page-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../talks>Talks</a></div class="topnav-page-selection"><div class=versions-dropdown><div class=topnav-page-selection><a href>Project</a> <i class="fa fa-chevron-down"></i></div class="topnav-page-selection"><div class=versions-dropdown-content><ul><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../community>Community</a></li class="topnav-page-selection"><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../spec>Spec</a></li class="topnav-page-selection"><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../view-spec>View Spec</a></li class="topnav-page-selection"><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../puffin-spec>Puffin Spec</a></li class="topnav-page-selection"><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../multi-engine-support>Multi-Engine Support</a></li class="topnav-page-selection"><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../how-to-release>How To Release</a></li class="topnav-page-selection"><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../terms>Terms</a></li class="topnav-page-selection"></ul></div></div><div class=versions-dropdown><div class=topnav-page-selection><a href>Concepts</a> <i class="fa fa-chevron-down"></i></div class="topnav-page-selection"><div class=versions-dropdown-content><ul><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../catalog>Catalogs</a></li class="topnav-page-selection"></ul></div></div><div class=versions-dropdown><div class=topnav-page-selection><a href>ASF</a> <i class="fa fa-chevron-down"></i></div class="topnav-page-selection"><div class=versions-dropdown-content><ul><li class=topnav-page-selection><a target=_blank href=https://www.apache.org/foundation/sponsorship.html>Donate</a></li class="topnav-page-selection"><li class=topnav-page-selection><a target=_blank href=https://www.apache.org/events/current-event.html>Events</a></li class="topnav-page-selection"><li class=topnav-page-selection><a target=_blank href=https://www.apache.org/licenses/>License</a></li class="topnav-page-selection"><li class=topnav-page-selection><a target=_blank href=https://www.apache.org/security/>Security</a></li class="topnav-page-selection"><li class=topnav-page-selection><a target=_blank href=https://www.apache.org/foundation/thanks.html>Sponsors</a></li class="topnav-page-selection"></ul></div></div><div class=topnav-page-selection><a href=https://github.com/apache/iceberg target=_blank><img src=https://iceberg.apache.org/docs/fd-update-latestt//img/GitHub-Mark.png target=_blank class=top-navbar-logo></a></div><div class=topnav-page-selection><a href=https://join.slack.com/t/apache-iceberg/shared_invite/zt-287g3akar-K9Oe_En5j1UL7Y_Ikpai3A target=_blank><img src=https://iceberg.apache.org/docs/fd-update-latestt//img/Slack_Mark_Web.png target=_blank class=top-navbar-logo></a></div></div></topsection></nav><section><div id=search-results-container><ul id=search-results></ul></div></section><body dir=ltr><section><div class="grid-container leftnav-and-toc"><div class="sidebar markdown-body"><div id=full><ul><li><a href=../><span>Introduction</span></a></li><li><a class="chevron-toggle collapsed" data-toggle=collapse data-parent=full href=#Tables><span>Tables</span>
<i class="fa fa-chevron-right"></i>
<i class="fa fa-chevron-down"></i></a></li><div id=Tables class=collapse><ul class=sub-menu><li><a href=../branching/>Branching and Tagging</a></li><li><a href=../configuration/>Configuration</a></li><li><a href=../evolution/>Evolution</a></li><li><a href=../maintenance/>Maintenance</a></li><li><a href=../metrics-reporting/>Metrics Reporting</a></li><li><a href=../partitioning/>Partitioning</a></li><li><a href=../performance/>Performance</a></li><li><a href=../reliability/>Reliability</a></li><li><a href=../schemas/>Schemas</a></li></ul></div><li><a class=chevron-toggle data-toggle=collapse data-parent=full href=#Spark><span>Spark</span>
<i class="fa fa-chevron-right"></i>
<i class="fa fa-chevron-down"></i></a></li><div id=Spark class="collapse in"><ul class=sub-menu><li><a href=../getting-started/>Getting Started</a></li><li><a href=../spark-configuration/>Configuration</a></li><li><a href=../spark-ddl/>DDL</a></li><li><a id=active href=../spark-procedures/>Procedures</a></li><li><a href=../spark-queries/>Queries</a></li><li><a href=../spark-structured-streaming/>Structured Streaming</a></li><li><a href=../spark-writes/>Writes</a></li></ul></div><li><a class="chevron-toggle collapsed" data-toggle=collapse data-parent=full href=#Flink><span>Flink</span>
<i class="fa fa-chevron-right"></i>
<i class="fa fa-chevron-down"></i></a></li><div id=Flink class=collapse><ul class=sub-menu><li><a href=../flink/>Flink Getting Started</a></li><li><a href=../flink-connector/>Flink Connector</a></li><li><a href=../flink-ddl/>Flink DDL</a></li><li><a href=../flink-queries/>Flink Queries</a></li><li><a href=../flink-writes/>Flink Writes</a></li><li><a href=../flink-actions/>Flink Actions</a></li><li><a href=../flink-configuration/>Flink Configuration</a></li></ul></div><li><a href=../hive/><span>Hive</span></a></li><li><a target=_blank href=https://trino.io/docs/current/connector/iceberg.html><span>Trino</span></a></li><li><a target=_blank href=https://clickhouse.com/docs/en/engines/table-engines/integrations/iceberg><span>ClickHouse</span></a></li><li><a target=_blank href=https://prestodb.io/docs/current/connector/iceberg.html><span>Presto</span></a></li><li><a target=_blank href=https://docs.dremio.com/data-formats/apache-iceberg/><span>Dremio</span></a></li><li><a target=_blank href=https://docs.starrocks.io/en-us/latest/data_source/catalog/iceberg_catalog><span>StarRocks</span></a></li><li><a target=_blank href=https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg.html><span>Amazon Athena</span></a></li><li><a target=_blank href=https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-iceberg-use-cluster.html><span>Amazon EMR</span></a></li><li><a target=_blank href=https://impala.apache.org/docs/build/html/topics/impala_iceberg.html><span>Impala</span></a></li><li><a target=_blank href=https://doris.apache.org/docs/dev/lakehouse/multi-catalog/iceberg><span>Doris</span></a></li><li><a class="chevron-toggle collapsed" data-toggle=collapse data-parent=full href=#Integrations><span>Integrations</span>
<i class="fa fa-chevron-right"></i>
<i class="fa fa-chevron-down"></i></a></li><div id=Integrations class=collapse><ul class=sub-menu><li><a href=../aws/>AWS</a></li><li><a href=../dell/>Dell</a></li><li><a href=../jdbc/>JDBC</a></li><li><a href=../nessie/>Nessie</a></li></ul></div><li><a class="chevron-toggle collapsed" data-toggle=collapse data-parent=full href=#API><span>API</span>
<i class="fa fa-chevron-right"></i>
<i class="fa fa-chevron-down"></i></a></li><div id=API class=collapse><ul class=sub-menu><li><a href=../java-api-quickstart/>Java Quickstart</a></li><li><a href=../api/>Java API</a></li><li><a href=../custom-catalog/>Java Custom Catalog</a></li></ul></div><li><a class="chevron-toggle collapsed" data-toggle=collapse data-parent=full href=#Migration><span>Migration</span>
<i class="fa fa-chevron-right"></i>
<i class="fa fa-chevron-down"></i></a></li><div id=Migration class=collapse><ul class=sub-menu><li><a href=../table-migration/>Overview</a></li><li><a href=../hive-migration/>Hive Migration</a></li><li><a href=../delta-lake-migration/>Delta Lake Migration</a></li></ul></div><li><a href=https://iceberg.apache.org/docs/fd-update-latestt/../../javadoc/latest><span>Javadoc</span></a></li><li><a target=_blank href=https://py.iceberg.apache.org/><span>PyIceberg</span></a></li></div></div><div id=content class=markdown-body><div class=margin-for-toc><h1 id=spark-procedures>Spark Procedures</h1><p>To use Iceberg in Spark, first configure <a href=../spark-configuration>Spark catalogs</a>. Stored procedures are only available when using <a href=../spark-configuration#sql-extensions>Iceberg SQL extensions</a> in Spark 3.</p><h2 id=usage>Usage</h2><p>Procedures can be used from any configured Iceberg catalog with <code>CALL</code>. All procedures are in the namespace <code>system</code>.</p><p><code>CALL</code> supports passing arguments by name (recommended) or by position. Mixing position and named arguments is not supported.</p><h3 id=named-arguments>Named arguments</h3><p>All procedure arguments are named. When passing arguments by name, arguments can be in any order and any optional argument can be omitted.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.procedure_name(arg_name_2 <span style=color:#f92672>=&gt;</span> arg_2, arg_name_1 <span style=color:#f92672>=&gt;</span> arg_1)
</span></span></code></pre></div><h3 id=positional-arguments>Positional arguments</h3><p>When passing arguments by position, only the ending arguments may be omitted if they are optional.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.procedure_name(arg_1, arg_2, ... arg_n)
</span></span></code></pre></div><h2 id=snapshot-management>Snapshot management</h2><h3 id=rollback_to_snapshot><code>rollback_to_snapshot</code></h3><p>Roll back a table to a specific snapshot ID.</p><p>To roll back to a specific time, use <a href=#rollback_to_timestamp><code>rollback_to_timestamp</code></a>.</p><div class=info>This procedure invalidates all cached Spark plans that reference the affected table.</div><h4 id=usage-1>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Name of the table to update</td></tr><tr><td><code>snapshot_id</code></td><td>✔️</td><td>long</td><td>Snapshot ID to rollback to</td></tr></tbody></table><h4 id=output>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>previous_snapshot_id</code></td><td>long</td><td>The current snapshot ID before the rollback</td></tr><tr><td><code>current_snapshot_id</code></td><td>long</td><td>The new current snapshot ID</td></tr></tbody></table><h4 id=example>Example</h4><p>Roll back table <code>db.sample</code> to snapshot ID <code>1</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.rollback_to_snapshot(<span style=color:#e6db74>&#39;db.sample&#39;</span>, <span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><h3 id=rollback_to_timestamp><code>rollback_to_timestamp</code></h3><p>Roll back a table to the snapshot that was current at some time.</p><div class=info>This procedure invalidates all cached Spark plans that reference the affected table.</div><h4 id=usage-2>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Name of the table to update</td></tr><tr><td><code>timestamp</code></td><td>✔️</td><td>timestamp</td><td>A timestamp to rollback to</td></tr></tbody></table><h4 id=output-1>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>previous_snapshot_id</code></td><td>long</td><td>The current snapshot ID before the rollback</td></tr><tr><td><code>current_snapshot_id</code></td><td>long</td><td>The new current snapshot ID</td></tr></tbody></table><h4 id=example-1>Example</h4><p>Roll back <code>db.sample</code> to a specific day and time.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.rollback_to_timestamp(<span style=color:#e6db74>&#39;db.sample&#39;</span>, <span style=color:#66d9ef>TIMESTAMP</span> <span style=color:#e6db74>&#39;2021-06-30 00:00:00.000&#39;</span>)
</span></span></code></pre></div><h3 id=set_current_snapshot><code>set_current_snapshot</code></h3><p>Sets the current snapshot ID for a table.</p><p>Unlike rollback, the snapshot is not required to be an ancestor of the current table state.</p><div class=info>This procedure invalidates all cached Spark plans that reference the affected table.</div><h4 id=usage-3>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Name of the table to update</td></tr><tr><td><code>snapshot_id</code></td><td></td><td>long</td><td>Snapshot ID to set as current</td></tr><tr><td><code>ref</code></td><td></td><td>string</td><td>Snapshot Referece (branch or tag) to set as current</td></tr></tbody></table><p>Either <code>snapshot_id</code> or <code>ref</code> must be provided but not both.</p><h4 id=output-2>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>previous_snapshot_id</code></td><td>long</td><td>The current snapshot ID before the rollback</td></tr><tr><td><code>current_snapshot_id</code></td><td>long</td><td>The new current snapshot ID</td></tr></tbody></table><h4 id=example-2>Example</h4><p>Set the current snapshot for <code>db.sample</code> to 1:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.set_current_snapshot(<span style=color:#e6db74>&#39;db.sample&#39;</span>, <span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p>Set the current snapshot for <code>db.sample</code> to tag <code>s1</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.set_current_snapshot(<span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.sample&#39;</span>, tag <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;s1&#39;</span>);
</span></span></code></pre></div><h3 id=cherrypick_snapshot><code>cherrypick_snapshot</code></h3><p>Cherry-picks changes from a snapshot into the current table state.</p><p>Cherry-picking creates a new snapshot from an existing snapshot without altering or removing the original.</p><p>Only append and dynamic overwrite snapshots can be cherry-picked.</p><div class=info>This procedure invalidates all cached Spark plans that reference the affected table.</div><h4 id=usage-4>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Name of the table to update</td></tr><tr><td><code>snapshot_id</code></td><td>✔️</td><td>long</td><td>The snapshot ID to cherry-pick</td></tr></tbody></table><h4 id=output-3>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>source_snapshot_id</code></td><td>long</td><td>The table&rsquo;s current snapshot before the cherry-pick</td></tr><tr><td><code>current_snapshot_id</code></td><td>long</td><td>The snapshot ID created by applying the cherry-pick</td></tr></tbody></table><h4 id=examples>Examples</h4><p>Cherry-pick snapshot 1</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.cherrypick_snapshot(<span style=color:#e6db74>&#39;my_table&#39;</span>, <span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p>Cherry-pick snapshot 1 with named args</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.cherrypick_snapshot(snapshot_id <span style=color:#f92672>=&gt;</span> <span style=color:#ae81ff>1</span>, <span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;my_table&#39;</span> )
</span></span></code></pre></div><h3 id=fast_forward><code>fast_forward</code></h3><p>Fast-forward the current snapshot of one branch to the latest snapshot of another.</p><h4 id=usage-5>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Name of the table to update</td></tr><tr><td><code>branch</code></td><td>✔️</td><td>string</td><td>Name of the branch to fast-forward</td></tr><tr><td><code>to</code></td><td>✔️</td><td>string</td><td></td></tr></tbody></table><h4 id=output-4>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>branch_updated</code></td><td>string</td><td>Name of the branch that has been fast-forwarded</td></tr><tr><td><code>previous_ref</code></td><td>long</td><td>The snapshot ID before applying fast-forward</td></tr><tr><td><code>updated_ref</code></td><td>long</td><td>The current snapshot ID after applying fast-forward</td></tr></tbody></table><h4 id=examples-1>Examples</h4><p>Fast-forward the main branch to the head of <code>audit-branch</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.fast_forward(<span style=color:#e6db74>&#39;my_table&#39;</span>, <span style=color:#e6db74>&#39;main&#39;</span>, <span style=color:#e6db74>&#39;audit-branch&#39;</span>)
</span></span></code></pre></div><h2 id=metadata-management>Metadata management</h2><p>Many <a href=../maintenance>maintenance actions</a> can be performed using Iceberg stored procedures.</p><h3 id=expire_snapshots><code>expire_snapshots</code></h3><p>Each write/update/delete/upsert/compaction in Iceberg produces a new snapshot while keeping the old data and metadata
around for snapshot isolation and time travel. The <code>expire_snapshots</code> procedure can be used to remove older snapshots
and their files which are no longer needed.</p><p>This procedure will remove old snapshots and data files which are uniquely required by those old snapshots. This means
the <code>expire_snapshots</code> procedure will never remove files which are still required by a non-expired snapshot.</p><h4 id=usage-6>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Name of the table to update</td></tr><tr><td><code>older_than</code></td><td>️</td><td>timestamp</td><td>Timestamp before which snapshots will be removed (Default: 5 days ago)</td></tr><tr><td><code>retain_last</code></td><td></td><td>int</td><td>Number of ancestor snapshots to preserve regardless of <code>older_than</code> (defaults to 1)</td></tr><tr><td><code>max_concurrent_deletes</code></td><td></td><td>int</td><td>Size of the thread pool used for delete file actions (by default, no thread pool is used)</td></tr><tr><td><code>stream_results</code></td><td></td><td>boolean</td><td>When true, deletion files will be sent to Spark driver by RDD partition (by default, all the files will be sent to Spark driver). This option is recommended to set to <code>true</code> to prevent Spark driver OOM from large file size</td></tr><tr><td><code>snapshot_ids</code></td><td></td><td>array of long</td><td>Array of snapshot IDs to expire.</td></tr></tbody></table><p>If <code>older_than</code> and <code>retain_last</code> are omitted, the table&rsquo;s <a href=../configuration/#table-behavior-properties>expiration properties</a> will be used.
Snapshots that are still referenced by branches or tags won&rsquo;t be removed. By default, branches and tags never expire, but their retention policy can be changed with the table property <code>history.expire.max-ref-age-ms</code>. The <code>main</code> branch never expires.</p><h4 id=output-5>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>deleted_data_files_count</code></td><td>long</td><td>Number of data files deleted by this operation</td></tr><tr><td><code>deleted_position_delete_files_count</code></td><td>long</td><td>Number of position delete files deleted by this operation</td></tr><tr><td><code>deleted_equality_delete_files_count</code></td><td>long</td><td>Number of equality delete files deleted by this operation</td></tr><tr><td><code>deleted_manifest_files_count</code></td><td>long</td><td>Number of manifest files deleted by this operation</td></tr><tr><td><code>deleted_manifest_lists_count</code></td><td>long</td><td>Number of manifest List files deleted by this operation</td></tr></tbody></table><h4 id=examples-2>Examples</h4><p>Remove snapshots older than specific day and time, but retain the last 100 snapshots:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> hive_prod.<span style=color:#66d9ef>system</span>.expire_snapshots(<span style=color:#e6db74>&#39;db.sample&#39;</span>, <span style=color:#66d9ef>TIMESTAMP</span> <span style=color:#e6db74>&#39;2021-06-30 00:00:00.000&#39;</span>, <span style=color:#ae81ff>100</span>)
</span></span></code></pre></div><p>Remove snapshots with snapshot ID <code>123</code> (note that this snapshot ID should not be the current snapshot):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> hive_prod.<span style=color:#66d9ef>system</span>.expire_snapshots(<span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.sample&#39;</span>, snapshot_ids <span style=color:#f92672>=&gt;</span> ARRAY(<span style=color:#ae81ff>123</span>))
</span></span></code></pre></div><h3 id=remove_orphan_files><code>remove_orphan_files</code></h3><p>Used to remove files which are not referenced in any metadata files of an Iceberg table and can thus be considered &ldquo;orphaned&rdquo;.</p><h4 id=usage-7>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Name of the table to clean</td></tr><tr><td><code>older_than</code></td><td>️</td><td>timestamp</td><td>Remove orphan files created before this timestamp (Defaults to 3 days ago)</td></tr><tr><td><code>location</code></td><td></td><td>string</td><td>Directory to look for files in (defaults to the table&rsquo;s location)</td></tr><tr><td><code>dry_run</code></td><td></td><td>boolean</td><td>When true, don&rsquo;t actually remove files (defaults to false)</td></tr><tr><td><code>max_concurrent_deletes</code></td><td></td><td>int</td><td>Size of the thread pool used for delete file actions (by default, no thread pool is used)</td></tr></tbody></table><h4 id=output-6>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>orphan_file_location</code></td><td>String</td><td>The path to each file determined to be an orphan by this command</td></tr></tbody></table><h4 id=examples-3>Examples</h4><p>List all the files that are candidates for removal by performing a dry run of the <code>remove_orphan_files</code> command on this table without actually removing them:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.remove_orphan_files(<span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.sample&#39;</span>, dry_run <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>true</span>)
</span></span></code></pre></div><p>Remove any files in the <code>tablelocation/data</code> folder which are not known to the table <code>db.sample</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.remove_orphan_files(<span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.sample&#39;</span>, <span style=color:#66d9ef>location</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;tablelocation/data&#39;</span>)
</span></span></code></pre></div><h3 id=rewrite_data_files><code>rewrite_data_files</code></h3><p>Iceberg tracks each data file in a table. More data files leads to more metadata stored in manifest files, and small data files causes an unnecessary amount of metadata and less efficient queries from file open costs.</p><p>Iceberg can compact data files in parallel using Spark with the <code>rewriteDataFiles</code> action. This will combine small files into larger files to reduce metadata overhead and runtime file open cost.</p><h4 id=usage-8>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Name of the table to update</td></tr><tr><td><code>strategy</code></td><td></td><td>string</td><td>Name of the strategy - binpack or sort. Defaults to binpack strategy</td></tr><tr><td><code>sort_order</code></td><td></td><td>string</td><td>For Zorder use a comma separated list of columns within zorder(). (Supported in Spark 3.2 and Above) Example: zorder(c1,c2,c3). Else, Comma separated sort orders in the format (ColumnName SortDirection NullOrder). Where SortDirection can be ASC or DESC. NullOrder can be NULLS FIRST or NULLS LAST. Defaults to the table&rsquo;s sort order</td></tr><tr><td><code>options</code></td><td>️</td><td>map&lt;string, string></td><td>Options to be used for actions</td></tr><tr><td><code>where</code></td><td>️</td><td>string</td><td>predicate as a string used for filtering the files. Note that all files that may contain data matching the filter will be selected for rewriting</td></tr></tbody></table><h4 id=options>Options</h4><h5 id=general-options>General Options</h5><table><thead><tr><th>Name</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td><code>max-concurrent-file-group-rewrites</code></td><td>5</td><td>Maximum number of file groups to be simultaneously rewritten</td></tr><tr><td><code>partial-progress.enabled</code></td><td>false</td><td>Enable committing groups of files prior to the entire rewrite completing</td></tr><tr><td><code>partial-progress.max-commits</code></td><td>10</td><td>Maximum amount of commits that this rewrite is allowed to produce if partial progress is enabled</td></tr><tr><td><code>use-starting-sequence-number</code></td><td>true</td><td>Use the sequence number of the snapshot at compaction start time instead of that of the newly produced snapshot</td></tr><tr><td><code>rewrite-job-order</code></td><td>none</td><td>Force the rewrite job order based on the value. If rewrite-job-order=bytes-asc, then rewrite the smallest job groups first.If rewrite-job-order=bytes-desc, then rewrite the largest job groups first.If rewrite-job-order=files-asc, then rewrite the job groups with the least files first.If rewrite-job-order=files-desc, then rewrite the job groups with the most files first.If rewrite-job-order=none, then rewrite job groups in the order they were planned (no specific ordering).</td></tr><tr><td><code>target-file-size-bytes</code></td><td>536870912 (512 MB, default value of <code>write.target-file-size-bytes</code> from <a href=../configuration/#write-properties>table properties</a>)</td><td>Target output file size</td></tr><tr><td><code>min-file-size-bytes</code></td><td>75% of target file size</td><td>Files under this threshold will be considered for rewriting regardless of any other criteria</td></tr><tr><td><code>max-file-size-bytes</code></td><td>180% of target file size</td><td>Files with sizes above this threshold will be considered for rewriting regardless of any other criteria</td></tr><tr><td><code>min-input-files</code></td><td>5</td><td>Any file group exceeding this number of files will be rewritten regardless of other criteria</td></tr><tr><td><code>rewrite-all</code></td><td>false</td><td>Force rewriting of all provided files overriding other options</td></tr><tr><td><code>max-file-group-size-bytes</code></td><td>107374182400 (100GB)</td><td>Largest amount of data that should be rewritten in a single file group. The entire rewrite operation is broken down into pieces based on partitioning and within partitions based on size into file-groups. This helps with breaking down the rewriting of very large partitions which may not be rewritable otherwise due to the resource constraints of the cluster.</td></tr><tr><td><code>delete-file-threshold</code></td><td>2147483647</td><td>Minimum number of deletes that needs to be associated with a data file for it to be considered for rewriting</td></tr></tbody></table><h5 id=options-for-sort-strategy>Options for sort strategy</h5><table><thead><tr><th>Name</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td><code>compression-factor</code></td><td>1.0</td><td>The number of shuffle partitions and consequently the number of output files created by the Spark sort is based on the size of the input data files used in this file rewriter. Due to compression, the disk file sizes may not accurately represent the size of files in the output. This parameter lets the user adjust the file size used for estimating actual output data size. A factor greater than 1.0 would generate more files than we would expect based on the on-disk file size. A value less than 1.0 would create fewer files than we would expect based on the on-disk size.</td></tr><tr><td><code>shuffle-partitions-per-file</code></td><td>1</td><td>Number of shuffle partitions to use for each output file. Iceberg will use a custom coalesce operation to stitch these sorted partitions back together into a single sorted file.</td></tr></tbody></table><h5 id=options-for-sort-strategy-with-zorder-sort_order>Options for sort strategy with zorder sort_order</h5><table><thead><tr><th>Name</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td><code>var-length-contribution</code></td><td>8</td><td>Number of bytes considered from an input column of a type with variable length (String, Binary)</td></tr><tr><td><code>max-output-size</code></td><td>2147483647</td><td>Amount of bytes interleaved in the ZOrder algorithm</td></tr></tbody></table><h4 id=output-7>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>rewritten_data_files_count</code></td><td>int</td><td>Number of data which were re-written by this command</td></tr><tr><td><code>added_data_files_count</code></td><td>int</td><td>Number of new data files which were written by this command</td></tr><tr><td><code>rewritten_bytes_count</code></td><td>long</td><td>Number of bytes which were written by this command</td></tr><tr><td><code>failed_data_files_count</code></td><td>int</td><td>Number of data files that failed to be rewritten when <code>partial-progress.enabled</code> is true</td></tr></tbody></table><h4 id=examples-4>Examples</h4><p>Rewrite the data files in table <code>db.sample</code> using the default rewrite algorithm of bin-packing to combine small files
and also split large files according to the default write size of the table.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.rewrite_data_files(<span style=color:#e6db74>&#39;db.sample&#39;</span>)
</span></span></code></pre></div><p>Rewrite the data files in table <code>db.sample</code> by sorting all the data on id and name
using the same defaults as bin-pack to determine which files to rewrite.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.rewrite_data_files(<span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.sample&#39;</span>, strategy <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;sort&#39;</span>, sort_order <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;id DESC NULLS LAST,name ASC NULLS FIRST&#39;</span>)
</span></span></code></pre></div><p>Rewrite the data files in table <code>db.sample</code> by zOrdering on column c1 and c2.
Using the same defaults as bin-pack to determine which files to rewrite.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.rewrite_data_files(<span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.sample&#39;</span>, strategy <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;sort&#39;</span>, sort_order <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;zorder(c1,c2)&#39;</span>)
</span></span></code></pre></div><p>Rewrite the data files in table <code>db.sample</code> using bin-pack strategy in any partition where more than 2 or more files need to be rewritten.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.rewrite_data_files(<span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.sample&#39;</span>, <span style=color:#66d9ef>options</span> <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>map</span>(<span style=color:#e6db74>&#39;min-input-files&#39;</span>,<span style=color:#e6db74>&#39;2&#39;</span>))
</span></span></code></pre></div><p>Rewrite the data files in table <code>db.sample</code> and select the files that may contain data matching the filter (id = 3 and name = &ldquo;foo&rdquo;) to be rewritten.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.rewrite_data_files(<span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.sample&#39;</span>, <span style=color:#66d9ef>where</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;id = 3 and name = &#34;foo&#34;&#39;</span>)
</span></span></code></pre></div><h3 id=rewrite_manifests><code>rewrite_manifests</code></h3><p>Rewrite manifests for a table to optimize scan planning.</p><p>Data files in manifests are sorted by fields in the partition spec. This procedure runs in parallel using a Spark job.</p><div class=info>This procedure invalidates all cached Spark plans that reference the affected table.</div><h4 id=usage-9>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Name of the table to update</td></tr><tr><td><code>use_caching</code></td><td>️</td><td>boolean</td><td>Use Spark caching during operation (defaults to true)</td></tr></tbody></table><h4 id=output-8>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>rewritten_manifests_count</code></td><td>int</td><td>Number of manifests which were re-written by this command</td></tr><tr><td><code>added_mainfests_count</code></td><td>int</td><td>Number of new manifest files which were written by this command</td></tr></tbody></table><h4 id=examples-5>Examples</h4><p>Rewrite the manifests in table <code>db.sample</code> and align manifest files with table partitioning.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.rewrite_manifests(<span style=color:#e6db74>&#39;db.sample&#39;</span>)
</span></span></code></pre></div><p>Rewrite the manifests in table <code>db.sample</code> and disable the use of Spark caching. This could be done to avoid memory issues on executors.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.rewrite_manifests(<span style=color:#e6db74>&#39;db.sample&#39;</span>, <span style=color:#66d9ef>false</span>)
</span></span></code></pre></div><h3 id=rewrite_position_delete_files><code>rewrite_position_delete_files</code></h3><p>Iceberg can rewrite position delete files, which serves two purposes:</p><ul><li>Minor Compaction: Compact small position delete files into larger ones. This reduces the size of metadata stored in manifest files and overhead of opening small delete files.</li><li>Remove Dangling Deletes: Filter out position delete records that refer to data files that are no longer live. After rewrite_data_files, position delete records pointing to the rewritten data files are not always marked for removal, and can remain tracked by the table&rsquo;s live snapshot metadata. This is known as the &lsquo;dangling delete&rsquo; problem.</li></ul><h4 id=usage-10>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Name of the table to update</td></tr><tr><td><code>options</code></td><td>️</td><td>map&lt;string, string></td><td>Options to be used for procedure</td></tr></tbody></table><p>Dangling deletes are always filtered out during rewriting.</p><h4 id=options-1>Options</h4><table><thead><tr><th>Name</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td><code>max-concurrent-file-group-rewrites</code></td><td>5</td><td>Maximum number of file groups to be simultaneously rewritten</td></tr><tr><td><code>partial-progress.enabled</code></td><td>false</td><td>Enable committing groups of files prior to the entire rewrite completing</td></tr><tr><td><code>partial-progress.max-commits</code></td><td>10</td><td>Maximum amount of commits that this rewrite is allowed to produce if partial progress is enabled</td></tr><tr><td><code>rewrite-job-order</code></td><td>none</td><td>Force the rewrite job order based on the value. If rewrite-job-order=bytes-asc, then rewrite the smallest job groups first.If rewrite-job-order=bytes-desc, then rewrite the largest job groups first.If rewrite-job-order=files-asc, then rewrite the job groups with the least files first.If rewrite-job-order=files-desc, then rewrite the job groups with the most files first.If rewrite-job-order=none, then rewrite job groups in the order they were planned (no specific ordering).</td></tr><tr><td><code>target-file-size-bytes</code></td><td>67108864 (64MB, default value of <code>write.delete.target-file-size-bytes</code> from <a href=../configuration/#write-properties>table properties</a>)</td><td>Target output file size</td></tr><tr><td><code>min-file-size-bytes</code></td><td>75% of target file size</td><td>Files under this threshold will be considered for rewriting regardless of any other criteria</td></tr><tr><td><code>max-file-size-bytes</code></td><td>180% of target file size</td><td>Files with sizes above this threshold will be considered for rewriting regardless of any other criteria</td></tr><tr><td><code>min-input-files</code></td><td>5</td><td>Any file group exceeding this number of files will be rewritten regardless of other criteria</td></tr><tr><td><code>rewrite-all</code></td><td>false</td><td>Force rewriting of all provided files overriding other options</td></tr><tr><td><code>max-file-group-size-bytes</code></td><td>107374182400 (100GB)</td><td>Largest amount of data that should be rewritten in a single file group. The entire rewrite operation is broken down into pieces based on partitioning and within partitions based on size into file-groups. This helps with breaking down the rewriting of very large partitions which may not be rewritable otherwise due to the resource constraints of the cluster.</td></tr></tbody></table><h4 id=output-9>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>rewritten_delete_files_count</code></td><td>int</td><td>Number of delete files which were removed by this command</td></tr><tr><td><code>added_delete_files_count</code></td><td>int</td><td>Number of delete files which were added by this command</td></tr><tr><td><code>rewritten_bytes_count</code></td><td>long</td><td>Count of bytes across delete files which were removed by this command</td></tr><tr><td><code>added_bytes_count</code></td><td>long</td><td>Count of bytes across all new delete files which were added by this command</td></tr></tbody></table><h4 id=examples-6>Examples</h4><p>Rewrite position delete files in table <code>db.sample</code>. This selects position delete files that fit default rewrite criteria, and writes new files of target size <code>target-file-size-bytes</code>. Dangling deletes are removed from rewritten delete files.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.rewrite_position_delete_files(<span style=color:#e6db74>&#39;db.sample&#39;</span>)
</span></span></code></pre></div><p>Rewrite all position delete files in table <code>db.sample</code>, writing new files <code>target-file-size-bytes</code>. Dangling deletes are removed from rewritten delete files.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.rewrite_position_delete_files(<span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.sample&#39;</span>, <span style=color:#66d9ef>options</span> <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>map</span>(<span style=color:#e6db74>&#39;rewrite-all&#39;</span>, <span style=color:#e6db74>&#39;true&#39;</span>))
</span></span></code></pre></div><p>Rewrite position delete files in table <code>db.sample</code>. This selects position delete files in partitions where 2 or more position delete files need to be rewritten based on size criteria. Dangling deletes are removed from rewritten delete files.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.rewrite_position_delete_files(<span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.sample&#39;</span>, <span style=color:#66d9ef>options</span> <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>map</span>(<span style=color:#e6db74>&#39;min-input-files&#39;</span>,<span style=color:#e6db74>&#39;2&#39;</span>))
</span></span></code></pre></div><h2 id=table-migration>Table migration</h2><p>The <code>snapshot</code> and <code>migrate</code> procedures help test and migrate existing Hive or Spark tables to Iceberg.</p><h3 id=snapshot><code>snapshot</code></h3><p>Create a light-weight temporary copy of a table for testing, without changing the source table.</p><p>The newly created table can be changed or written to without affecting the source table, but the snapshot uses the original table&rsquo;s data files.</p><p>When inserts or overwrites run on the snapshot, new files are placed in the snapshot table&rsquo;s location rather than the original table location.</p><p>When finished testing a snapshot table, clean it up by running <code>DROP TABLE</code>.</p><div class=info>Because tables created by <code>snapshot</code> are not the sole owners of their data files, they are prohibited from
actions like <code>expire_snapshots</code> which would physically delete data files. Iceberg deletes, which only effect metadata,
are still allowed. In addition, any operations which affect the original data files will disrupt the Snapshot&rsquo;s
integrity. DELETE statements executed against the original Hive table will remove original data files and the
<code>snapshot</code> table will no longer be able to access them.</div><p>See <a href=#migrate><code>migrate</code></a> to replace an existing table with an Iceberg table.</p><h4 id=usage-11>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>source_table</code></td><td>✔️</td><td>string</td><td>Name of the table to snapshot</td></tr><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Name of the new Iceberg table to create</td></tr><tr><td><code>location</code></td><td></td><td>string</td><td>Table location for the new table (delegated to the catalog by default)</td></tr><tr><td><code>properties</code></td><td>️</td><td>map&lt;string, string></td><td>Properties to add to the newly created table</td></tr></tbody></table><h4 id=output-10>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>imported_files_count</code></td><td>long</td><td>Number of files added to the new table</td></tr></tbody></table><h4 id=examples-7>Examples</h4><p>Make an isolated Iceberg table which references table <code>db.sample</code> named <code>db.snap</code> at the
catalog&rsquo;s default location for <code>db.snap</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.snapshot(<span style=color:#e6db74>&#39;db.sample&#39;</span>, <span style=color:#e6db74>&#39;db.snap&#39;</span>)
</span></span></code></pre></div><p>Migrate an isolated Iceberg table which references table <code>db.sample</code> named <code>db.snap</code> at
a manually specified location <code>/tmp/temptable/</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.snapshot(<span style=color:#e6db74>&#39;db.sample&#39;</span>, <span style=color:#e6db74>&#39;db.snap&#39;</span>, <span style=color:#e6db74>&#39;/tmp/temptable/&#39;</span>)
</span></span></code></pre></div><h3 id=migrate><code>migrate</code></h3><p>Replace a table with an Iceberg table, loaded with the source&rsquo;s data files.</p><p>Table schema, partitioning, properties, and location will be copied from the source table.</p><p>Migrate will fail if any table partition uses an unsupported format. Supported formats are Avro, Parquet, and ORC.
Existing data files are added to the Iceberg table&rsquo;s metadata and can be read using a name-to-id mapping created from the original table schema.</p><p>To leave the original table intact while testing, use <a href=#snapshot><code>snapshot</code></a> to create new temporary table that shares source data files and schema.</p><p>By default, the original table is retained with the name <code>table_BACKUP_</code>.</p><h4 id=usage-12>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Name of the table to migrate</td></tr><tr><td><code>properties</code></td><td>️</td><td>map&lt;string, string></td><td>Properties for the new Iceberg table</td></tr><tr><td><code>drop_backup</code></td><td></td><td>boolean</td><td>When true, the original table will not be retained as backup (defaults to false)</td></tr><tr><td><code>backup_table_name</code></td><td></td><td>string</td><td>Name of the table that will be retained as backup (defaults to <code>table_BACKUP_</code>)</td></tr></tbody></table><h4 id=output-11>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>migrated_files_count</code></td><td>long</td><td>Number of files appended to the Iceberg table</td></tr></tbody></table><h4 id=examples-8>Examples</h4><p>Migrate the table <code>db.sample</code> in Spark&rsquo;s default catalog to an Iceberg table and add a property &lsquo;foo&rsquo; set to &lsquo;bar&rsquo;:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.migrate(<span style=color:#e6db74>&#39;spark_catalog.db.sample&#39;</span>, <span style=color:#66d9ef>map</span>(<span style=color:#e6db74>&#39;foo&#39;</span>, <span style=color:#e6db74>&#39;bar&#39;</span>))
</span></span></code></pre></div><p>Migrate <code>db.sample</code> in the current catalog to an Iceberg table without adding any additional properties:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> <span style=color:#66d9ef>catalog_name</span>.<span style=color:#66d9ef>system</span>.migrate(<span style=color:#e6db74>&#39;db.sample&#39;</span>)
</span></span></code></pre></div><h3 id=add_files><code>add_files</code></h3><p>Attempts to directly add files from a Hive or file based table into a given Iceberg table. Unlike migrate or
snapshot, <code>add_files</code> can import files from a specific partition or partitions and does not create a new Iceberg table.
This command will create metadata for the new files and will not move them. This procedure will not analyze the schema
of the files to determine if they actually match the schema of the Iceberg table. Upon completion, the Iceberg table
will then treat these files as if they are part of the set of files owned by Iceberg. This means any subsequent
<code>expire_snapshot</code> calls will be able to physically delete the added files. This method should not be used if
<code>migrate</code> or <code>snapshot</code> are possible.</p><div class=warning>Keep in mind the <code>add_files</code> procedure will fetch the Parquet metadata from each file being added just once. If you&rsquo;re using tiered storage, (such as <a href=https://aws.amazon.com/s3/storage-classes/intelligent-tiering/>Amazon S3 Intelligent-Tiering storage class</a>), the underlying, file will be retrieved from the archive, and will remain on a higher tier for a set period of time.</div><h4 id=usage-13>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Table which will have files added to</td></tr><tr><td><code>source_table</code></td><td>✔️</td><td>string</td><td>Table where files should come from, paths are also possible in the form of `file_format`.`path`</td></tr><tr><td><code>partition_filter</code></td><td>️</td><td>map&lt;string, string></td><td>A map of partitions in the source table to import from</td></tr><tr><td><code>check_duplicate_files</code></td><td>️</td><td>boolean</td><td>Whether to prevent files existing in the table from being added (defaults to true)</td></tr></tbody></table><p>Warning : Schema is not validated, adding files with different schema to the Iceberg table will cause issues.</p><p>Warning : Files added by this method can be physically deleted by Iceberg operations</p><h4 id=output-12>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>added_files_count</code></td><td>long</td><td>The number of files added by this command</td></tr><tr><td><code>changed_partition_count</code></td><td>long</td><td>The number of partitioned changed by this command</td></tr></tbody></table><div class=warning>changed_partition_count will be 0 when table property <code>compatibility.snapshot-id-inheritance.enabled</code> is set to true</div><h4 id=examples-9>Examples</h4><p>Add the files from table <code>db.src_table</code>, a Hive or Spark table registered in the session Catalog, to Iceberg table
<code>db.tbl</code>. Only add files that exist within partitions where <code>part_col_1</code> is equal to <code>A</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> spark_catalog.<span style=color:#66d9ef>system</span>.add_files(
</span></span><span style=display:flex><span><span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.tbl&#39;</span>,
</span></span><span style=display:flex><span>source_table <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.src_tbl&#39;</span>,
</span></span><span style=display:flex><span>partition_filter <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>map</span>(<span style=color:#e6db74>&#39;part_col_1&#39;</span>, <span style=color:#e6db74>&#39;A&#39;</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Add files from a <code>parquet</code> file based table at location <code>path/to/table</code> to the Iceberg table <code>db.tbl</code>. Add all
files regardless of what partition they belong to.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> spark_catalog.<span style=color:#66d9ef>system</span>.add_files(
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.tbl&#39;</span>,
</span></span><span style=display:flex><span>  source_table <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;`parquet`.`path/to/table`&#39;</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h3 id=register_table><code>register_table</code></h3><p>Creates a catalog entry for a metadata.json file which already exists but does not have a corresponding catalog identifier.</p><h4 id=usage-14>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Table which is to be registered</td></tr><tr><td><code>metadata_file</code></td><td>✔️</td><td>string</td><td>Metadata file which is to be registered as a new catalog identifier</td></tr></tbody></table><div class=warning>Having the same metadata.json registered in more than one catalog can lead to missing updates, loss of data, and table corruption.
Only use this procedure when the table is no longer registered in an existing catalog, or you are moving a table between catalogs.</div><h4 id=output-13>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>current_snapshot_id</code></td><td>long</td><td>The current snapshot ID of the newly registered Iceberg table</td></tr><tr><td><code>total_records_count</code></td><td>long</td><td>Total records count of the newly registered Iceberg table</td></tr><tr><td><code>total_data_files_count</code></td><td>long</td><td>Total data files count of the newly registered Iceberg table</td></tr></tbody></table><h4 id=examples-10>Examples</h4><p>Register a new table as <code>db.tbl</code> to <code>spark_catalog</code> pointing to metadata.json file <code>path/to/metadata/file.json</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> spark_catalog.<span style=color:#66d9ef>system</span>.register_table(
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.tbl&#39;</span>,
</span></span><span style=display:flex><span>  metadata_file <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;path/to/metadata/file.json&#39;</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h2 id=metadata-information>Metadata information</h2><h3 id=ancestors_of><code>ancestors_of</code></h3><p>Report the live snapshot IDs of parents of a specified snapshot</p><h4 id=usage-15>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Name of the table to report live snapshot IDs</td></tr><tr><td><code>snapshot_id</code></td><td>️</td><td>long</td><td>Use a specified snapshot to get the live snapshot IDs of parents</td></tr></tbody></table><blockquote><p>tip : Using snapshot_id</p><p>Given snapshots history with roll back to B and addition of C&rsquo; -> D&rsquo;</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>A -&gt; B - &gt; C -&gt; D
</span></span><span style=display:flex><span>      <span style=color:#ae81ff>\ </span>-&gt; C<span style=color:#e6db74>&#39; -&gt; (D&#39;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Not specifying the snapshot ID would return A -> B -> C&rsquo; -> D&rsquo;, while providing the snapshot ID of
D as an argument would return A-> B -> C -> D</p></blockquote><h4 id=output-14>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>snapshot_id</code></td><td>long</td><td>the ancestor snapshot id</td></tr><tr><td><code>timestamp</code></td><td>long</td><td>snapshot creation time</td></tr></tbody></table><h4 id=examples-11>Examples</h4><p>Get all the snapshot ancestors of current snapshots(default)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> spark_catalog.<span style=color:#66d9ef>system</span>.ancestors_of(<span style=color:#e6db74>&#39;db.tbl&#39;</span>)
</span></span></code></pre></div><p>Get all the snapshot ancestors by a particular snapshot</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> spark_catalog.<span style=color:#66d9ef>system</span>.ancestors_of(<span style=color:#e6db74>&#39;db.tbl&#39;</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>CALL</span> spark_catalog.<span style=color:#66d9ef>system</span>.ancestors_of(snapshot_id <span style=color:#f92672>=&gt;</span> <span style=color:#ae81ff>1</span>, <span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.tbl&#39;</span>)
</span></span></code></pre></div><h2 id=change-data-capture>Change Data Capture</h2><h3 id=create_changelog_view><code>create_changelog_view</code></h3><p>Creates a view that contains the changes from a given table.</p><h4 id=usage-16>Usage</h4><table><thead><tr><th>Argument Name</th><th>Required?</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>table</code></td><td>✔️</td><td>string</td><td>Name of the source table for the changelog</td></tr><tr><td><code>changelog_view</code></td><td></td><td>string</td><td>Name of the view to create</td></tr><tr><td><code>options</code></td><td></td><td>map&lt;string, string></td><td>A map of Spark read options to use</td></tr><tr><td><code>net_changes</code></td><td></td><td>boolean</td><td>Whether to output net changes (see below for more information). Defaults to false.</td></tr><tr><td><code>compute_updates</code></td><td></td><td>boolean</td><td>Whether to compute pre/post update images (see below for more information). Defaults to false.</td></tr><tr><td><code>identifier_columns</code></td><td></td><td>array</td><td>The list of identifier columns to compute updates. If the argument <code>compute_updates</code> is set to true and <code>identifier_columns</code> are not provided, the table’s current identifier fields will be used.</td></tr><tr><td><code>remove_carryovers</code></td><td></td><td>boolean</td><td>Whether to remove carry-over rows (see below for more information). Defaults to true. Deprecated since 1.4.0, will be removed in 1.5.0; Please query <code>SparkChangelogTable</code> to view carry-over rows.</td></tr></tbody></table><p>Here is a list of commonly used Spark read options:</p><ul><li><code>start-snapshot-id</code>: the exclusive start snapshot ID. If not provided, it reads from the table’s first snapshot inclusively.</li><li><code>end-snapshot-id</code>: the inclusive end snapshot id, default to table&rsquo;s current snapshot.</li><li><code>start-timestamp</code>: the exclusive start timestamp. If not provided, it reads from the table’s first snapshot inclusively.</li><li><code>end-timestamp</code>: the inclusive end timestamp, default to table&rsquo;s current snapshot.</li></ul><h4 id=output-15>Output</h4><table><thead><tr><th>Output Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>changelog_view</code></td><td>string</td><td>The name of the created changelog view</td></tr></tbody></table><h4 id=examples-12>Examples</h4><p>Create a changelog view <code>tbl_changes</code> based on the changes that happened between snapshot <code>1</code> (exclusive) and <code>2</code> (inclusive).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> spark_catalog.<span style=color:#66d9ef>system</span>.create_changelog_view(
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.tbl&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>options</span> <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>map</span>(<span style=color:#e6db74>&#39;start-snapshot-id&#39;</span>,<span style=color:#e6db74>&#39;1&#39;</span>,<span style=color:#e6db74>&#39;end-snapshot-id&#39;</span>, <span style=color:#e6db74>&#39;2&#39;</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Create a changelog view <code>my_changelog_view</code> based on the changes that happened between timestamp <code>1678335750489</code> (exclusive) and <code>1678992105265</code> (inclusive).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> spark_catalog.<span style=color:#66d9ef>system</span>.create_changelog_view(
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.tbl&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>options</span> <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>map</span>(<span style=color:#e6db74>&#39;start-timestamp&#39;</span>,<span style=color:#e6db74>&#39;1678335750489&#39;</span>,<span style=color:#e6db74>&#39;end-timestamp&#39;</span>, <span style=color:#e6db74>&#39;1678992105265&#39;</span>),
</span></span><span style=display:flex><span>  changelog_view <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;my_changelog_view&#39;</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Create a changelog view that computes updates based on the identifier columns <code>id</code> and <code>name</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> spark_catalog.<span style=color:#66d9ef>system</span>.create_changelog_view(
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.tbl&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>options</span> <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>map</span>(<span style=color:#e6db74>&#39;start-snapshot-id&#39;</span>,<span style=color:#e6db74>&#39;1&#39;</span>,<span style=color:#e6db74>&#39;end-snapshot-id&#39;</span>, <span style=color:#e6db74>&#39;2&#39;</span>),
</span></span><span style=display:flex><span>  identifier_columns <span style=color:#f92672>=&gt;</span> array(<span style=color:#e6db74>&#39;id&#39;</span>, <span style=color:#e6db74>&#39;name&#39;</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Once the changelog view is created, you can query the view to see the changes that happened between the snapshots.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> tbl_changes
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> tbl_changes <span style=color:#66d9ef>where</span> _change_type <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;INSERT&#39;</span> <span style=color:#66d9ef>AND</span> id <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span> <span style=color:#66d9ef>ORDER</span> <span style=color:#66d9ef>BY</span> _change_ordinal
</span></span></code></pre></div><p>Please note that the changelog view includes Change Data Capture(CDC) metadata columns
that provide additional information about the changes being tracked. These columns are:</p><ul><li><code>_change_type</code>: the type of change. It has one of the following values: <code>INSERT</code>, <code>DELETE</code>, <code>UPDATE_BEFORE</code>, or <code>UPDATE_AFTER</code>.</li><li><code>_change_ordinal</code>: the order of changes</li><li><code>_commit_snapshot_id</code>: the snapshot ID where the change occurred</li></ul><p>Here is an example of corresponding results. It shows that the first snapshot inserted 2 records, and the
second snapshot deleted 1 record.</p><table><thead><tr><th>id</th><th>name</th><th>_change_type</th><th>_change_ordinal</th><th>_change_snapshot_id</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>INSERT</td><td>0</td><td>5390529835796506035</td></tr><tr><td>2</td><td>Bob</td><td>INSERT</td><td>0</td><td>5390529835796506035</td></tr><tr><td>1</td><td>Alice</td><td>DELETE</td><td>1</td><td>8764748981452218370</td></tr></tbody></table><p>Create a changelog view that computes net changes. It removes intermediate changes and only outputs the net changes.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CALL</span> spark_catalog.<span style=color:#66d9ef>system</span>.create_changelog_view(
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>table</span> <span style=color:#f92672>=&gt;</span> <span style=color:#e6db74>&#39;db.tbl&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>options</span> <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>map</span>(<span style=color:#e6db74>&#39;end-snapshot-id&#39;</span>, <span style=color:#e6db74>&#39;87647489814522183702&#39;</span>),
</span></span><span style=display:flex><span>  net_changes <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>With the net changes, the above changelog view only contains the following row since Alice was inserted in the first snapshot and deleted in the second snapshot.</p><table><thead><tr><th>id</th><th>name</th><th>_change_type</th><th>_change_ordinal</th><th>_change_snapshot_id</th></tr></thead><tbody><tr><td>2</td><td>Bob</td><td>INSERT</td><td>0</td><td>5390529835796506035</td></tr></tbody></table><h4 id=carry-over-rows>Carry-over Rows</h4><p>The procedure removes the carry-over rows by default. Carry-over rows are the result of row-level operations(<code>MERGE</code>, <code>UPDATE</code> and <code>DELETE</code>)
when using copy-on-write. For example, given a file which contains row1 <code>(id=1, name='Alice')</code> and row2 <code>(id=2, name='Bob')</code>.
A copy-on-write delete of row2 would require erasing this file and preserving row1 in a new file. The changelog table
reports this as the following pair of rows, despite it not being an actual change to the table.</p><table><thead><tr><th>id</th><th>name</th><th>_change_type</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>DELETE</td></tr><tr><td>1</td><td>Alice</td><td>INSERT</td></tr></tbody></table><p>To see carry-over rows, query <code>SparkChangelogTable</code> as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> spark_catalog.db.tbl.changes
</span></span></code></pre></div><h4 id=prepost-update-images>Pre/Post Update Images</h4><p>The procedure computes the pre/post update images if configured. Pre/post update images are converted from a
pair of a delete row and an insert row. Identifier columns are used for determining whether an insert and a delete record
refer to the same row. If the two records share the same values for the identity columns they are considered to be before
and after states of the same row. You can either set identifier fields in the table schema or input them as the procedure parameters.</p><p>The following example shows pre/post update images computation with an identifier column(<code>id</code>), where a row deletion
and an insertion with the same <code>id</code> are treated as a single update operation. Specifically, suppose we have the following pair of rows:</p><table><thead><tr><th>id</th><th>name</th><th>_change_type</th></tr></thead><tbody><tr><td>3</td><td>Robert</td><td>DELETE</td></tr><tr><td>3</td><td>Dan</td><td>INSERT</td></tr></tbody></table><p>In this case, the procedure marks the row before the update as an <code>UPDATE_BEFORE</code> image and the row after the update
as an <code>UPDATE_AFTER</code> image, resulting in the following pre/post update images:</p><table><thead><tr><th>id</th><th>name</th><th>_change_type</th></tr></thead><tbody><tr><td>3</td><td>Robert</td><td>UPDATE_BEFORE</td></tr><tr><td>3</td><td>Dan</td><td>UPDATE_AFTER</td></tr></tbody></table></div><div id=toc class=markdown-body><div id=full><nav id=TableOfContents><ul><li><a href=#usage>Usage</a><ul><li><a href=#named-arguments>Named arguments</a></li><li><a href=#positional-arguments>Positional arguments</a></li></ul></li><li><a href=#snapshot-management>Snapshot management</a><ul><li><a href=#rollback_to_snapshot><code>rollback_to_snapshot</code></a></li><li><a href=#rollback_to_timestamp><code>rollback_to_timestamp</code></a></li><li><a href=#set_current_snapshot><code>set_current_snapshot</code></a></li><li><a href=#cherrypick_snapshot><code>cherrypick_snapshot</code></a></li><li><a href=#fast_forward><code>fast_forward</code></a></li></ul></li><li><a href=#metadata-management>Metadata management</a><ul><li><a href=#expire_snapshots><code>expire_snapshots</code></a></li><li><a href=#remove_orphan_files><code>remove_orphan_files</code></a></li><li><a href=#rewrite_data_files><code>rewrite_data_files</code></a></li><li><a href=#rewrite_manifests><code>rewrite_manifests</code></a></li><li><a href=#rewrite_position_delete_files><code>rewrite_position_delete_files</code></a></li></ul></li><li><a href=#table-migration>Table migration</a><ul><li><a href=#snapshot><code>snapshot</code></a></li><li><a href=#migrate><code>migrate</code></a></li><li><a href=#add_files><code>add_files</code></a></li><li><a href=#register_table><code>register_table</code></a></li></ul></li><li><a href=#metadata-information>Metadata information</a><ul><li><a href=#ancestors_of><code>ancestors_of</code></a></li></ul></li><li><a href=#change-data-capture>Change Data Capture</a><ul><li><a href=#create_changelog_view><code>create_changelog_view</code></a></li></ul></li></ul></nav></div></div></div></div></section></body><script src=https://iceberg.apache.org/docs/fd-update-latestt//js/jquery-1.11.0.js></script><script src=https://iceberg.apache.org/docs/fd-update-latestt//js/jquery.easing.min.js></script><script type=text/javascript src=https://iceberg.apache.org/docs/fd-update-latestt//js/search.js></script><script src=https://iceberg.apache.org/docs/fd-update-latestt//js/bootstrap.min.js></script><script src=https://iceberg.apache.org/docs/fd-update-latestt//js/iceberg-theme.js></script></html>