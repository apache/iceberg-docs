<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Apache Iceberg</title><link>https://iceberg.apache.org/docs/0.12.1/docs/flink/</link><description>Recent content on Apache Iceberg</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://iceberg.apache.org/docs/0.12.1/docs/flink/index.xml" rel="self" type="application/rss+xml"/><item><title>Getting Started</title><link>https://iceberg.apache.org/docs/0.12.1/flink/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://iceberg.apache.org/docs/0.12.1/flink/</guid><description>Flink # Apache Iceberg supports both Apache Flink&amp;rsquo;s DataStream API and Table API to write records into an Iceberg table. Currently, we only integrate Iceberg with Apache Flink 1.11.x.
Feature support Flink 1.11.0 Notes SQL create catalog ✔️ SQL create database ✔️ SQL create table ✔️ SQL create table like ✔️ SQL alter table ✔️ Only support altering table properties, Columns/PartitionKey changes are not supported now SQL drop_table ✔️ SQL select ✔️ Support both streaming and batch mode SQL insert into ✔️ ️ Support both streaming and batch mode SQL insert overwrite ✔️ ️ DataStream read ✔️ ️ DataStream append ✔️ ️ DataStream overwrite ✔️ ️ Metadata tables ️ Support Java API but does not support Flink SQL Rewrite files action ✔️ ️ Preparation when using Flink SQL Client # To create iceberg table in flink, we recommend to use Flink SQL Client because it&amp;rsquo;s easier for users to understand the concepts.</description></item><item><title/><link>https://iceberg.apache.org/docs/0.12.1/flink-connector/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://iceberg.apache.org/docs/0.12.1/flink-connector/</guid><description>Flink Connector # Apache Flink supports creating Iceberg table directly without creating the explicit Flink catalog in Flink SQL. That means we can just create an iceberg table by specifying 'connector'='iceberg' table option in Flink SQL which is similar to usage in the Flink official document.
In Flink, the SQL CREATE TABLE test (..) WITH ('connector'='iceberg', ...) will create a Flink table in current Flink catalog (use GenericInMemoryCatalog by default), which is just mapping to the underlying iceberg table instead of maintaining iceberg table directly in current Flink catalog.</description></item></channel></rss>