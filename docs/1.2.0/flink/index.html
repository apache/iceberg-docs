<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=author content><title>Enabling Iceberg in Flink</title><link href=../css/bootstrap.css rel=stylesheet><link href=../css/markdown.css rel=stylesheet><link href=../css/katex.min.css rel=stylesheet><link href=../css/iceberg-theme.css rel=stylesheet><link href=../font-awesome-4.7.0/css/font-awesome.min.css rel=stylesheet type=text/css><link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel=stylesheet type=text/css><link href=../css/termynal.css rel=stylesheet></head><body><head><script>function addAnchor(e){e.insertAdjacentHTML("beforeend",`<a href="#${e.id}" class="anchortag" ariaLabel="Anchor"> üîó </a>`)}document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll("h1[id], h2[id], h3[id], h4[id]");e&&e.forEach(addAnchor)})</script></head><nav class="navbar navbar-default" role=navigation><topsection><div class=navbar-fixed-top><div><button type=button class=navbar-toggle data-toggle=collapse data-target=div.sidebar>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button>
<a class="page-scroll navbar-brand" href=https://iceberg.apache.org/><img class=top-navbar-logo src=https://iceberg.apache.org/docs/1.2.0//img/iceberg-logo-icon.png> Apache Iceberg</a></div><div><input type=search class=form-control id=search-input placeholder=Search... maxlength=64 data-hotkeys=s/></div><div class=versions-dropdown><span>1.2.0</span> <i class="fa fa-chevron-down"></i><div class=versions-dropdown-content><ul><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/1.2.0/../latest>latest</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/1.2.0/../1.2.0>1.2.0</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/1.2.0/../1.1.0>1.1.0</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/1.2.0/../1.0.0>1.0.0</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/1.2.0/../0.14.1>0.14.1</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/1.2.0/../0.14.0>0.14.0</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/1.2.0/../0.13.2>0.13.2</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/1.2.0/../0.13.1>0.13.1</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/1.2.0/../0.13.0>0.13.0</a></li><li class=versions-dropdown-selection><a href=https://iceberg.apache.org/docs/1.2.0/../0.12.1>0.12.1</a></li></ul></div></div></div><div class="navbar-menu-fixed-top navbar-pages-group"><div class=topnav-page-selection><a href=https://iceberg.apache.org/docs/1.2.0/../../spark-quickstart>Quickstart</a></div class="topnav-page-selection"><div class=topnav-page-selection><a id=active href=https://iceberg.apache.org/docs/1.2.0/../../docs/latest>Docs</a></div><div class=topnav-page-selection><a href=https://iceberg.apache.org/docs/1.2.0/../../releases>Releases</a></div class="topnav-page-selection"><div class=topnav-page-selection><a href=https://iceberg.apache.org/docs/1.2.0/../../roadmap>Roadmap</a></div class="topnav-page-selection"><div class=topnav-page-selection><a href=https://iceberg.apache.org/docs/1.2.0/../../blogs>Blogs</a></div class="topnav-page-selection"><div class=topnav-page-selection><a href=https://iceberg.apache.org/docs/1.2.0/../../talks>Talks</a></div class="topnav-page-selection"><div class=versions-dropdown><div class=topnav-page-selection><a href>Project</a> <i class="fa fa-chevron-down"></i></div class="topnav-page-selection"><div class=versions-dropdown-content><ul><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/1.2.0/../../community>Community</a></li class="topnav-page-selection"><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/1.2.0/../../spec>Spec</a></li class="topnav-page-selection"><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/1.2.0/../../view-spec>View Spec</a></li class="topnav-page-selection"><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/1.2.0/../../puffin-spec>Puffin Spec</a></li class="topnav-page-selection"><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/1.2.0/../../multi-engine-support>Multi-Engine Support</a></li class="topnav-page-selection"><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/1.2.0/../../how-to-release>How To Release</a></li class="topnav-page-selection"><li class=topnav-page-selection><a href=https://iceberg.apache.org/docs/1.2.0/../../terms>Terms</a></li class="topnav-page-selection"></ul></div></div><div class=versions-dropdown><div class=topnav-page-selection><a href>ASF</a> <i class="fa fa-chevron-down"></i></div class="topnav-page-selection"><div class=versions-dropdown-content><ul><li class=topnav-page-selection><a target=_blank href=https://www.apache.org/foundation/sponsorship.html>Donate</a></li class="topnav-page-selection"><li class=topnav-page-selection><a target=_blank href=https://www.apache.org/events/current-event.html>Events</a></li class="topnav-page-selection"><li class=topnav-page-selection><a target=_blank href=https://www.apache.org/licenses/>License</a></li class="topnav-page-selection"><li class=topnav-page-selection><a target=_blank href=https://www.apache.org/security/>Security</a></li class="topnav-page-selection"><li class=topnav-page-selection><a target=_blank href=https://www.apache.org/foundation/thanks.html>Sponsors</a></li class="topnav-page-selection"></ul></div></div><div class=topnav-page-selection><a href=https://github.com/apache/iceberg target=_blank><img src=https://iceberg.apache.org/docs/1.2.0//img/GitHub-Mark.png target=_blank class=top-navbar-logo></a></div><div class=topnav-page-selection><a href=https://join.slack.com/t/apache-iceberg/shared_invite/zt-1oj35f7yc-wuTEhvkiqjGLje83B7rG8A target=_blank><img src=https://iceberg.apache.org/docs/1.2.0//img/Slack_Mark_Web.png target=_blank class=top-navbar-logo></a></div></div></topsection></nav><section><div id=search-results-container><ul id=search-results></ul></div></section><body dir=" ltr"><section><div class="grid-container leftnav-and-toc"><div class="sidebar markdown-body"><div id=full><ul><li><a href=../><span>Introduction</span></a></li><li><a class="chevron-toggle collapsed" data-toggle=collapse data-parent=full href=#Tables><span>Tables</span>
<i class="fa fa-chevron-right"></i>
<i class="fa fa-chevron-down"></i></a></li><div id=Tables class=collapse><ul class=sub-menu><li><a href=../configuration/>Configuration</a></li><li><a href=../evolution/>Evolution</a></li><li><a href=../maintenance/>Maintenance</a></li><li><a href=../partitioning/>Partitioning</a></li><li><a href=../performance/>Performance</a></li><li><a href=../reliability/>Reliability</a></li><li><a href=../schemas/>Schemas</a></li></ul></div><li><a class="chevron-toggle collapsed" data-toggle=collapse data-parent=full href=#Spark><span>Spark</span>
<i class="fa fa-chevron-right"></i>
<i class="fa fa-chevron-down"></i></a></li><div id=Spark class=collapse><ul class=sub-menu><li><a href=../spark-ddl/>DDL</a></li><li><a href=../getting-started/>Getting Started</a></li><li><a href=../spark-procedures/>Procedures</a></li><li><a href=../spark-queries/>Queries</a></li><li><a href=../spark-structured-streaming/>Structured Streaming</a></li><li><a href=../spark-writes/>Writes</a></li></ul></div><li><a class=chevron-toggle data-toggle=collapse data-parent=full href=#Flink><span>Flink</span>
<i class="fa fa-chevron-right"></i>
<i class="fa fa-chevron-down"></i></a></li><div id=Flink class="collapse in"><ul class=sub-menu><li><a id=active href=../flink/>Enabling Iceberg in Flink</a></li><li><a href=../flink-connector/>Flink Connector</a></li></ul></div><li><a href=../hive/><span>Hive</span></a></li><li><a target=_blank href=https://trino.io/docs/current/connector/iceberg.html><span>Trino</span></a></li><li><a target=_blank href=https://prestodb.io/docs/current/connector/iceberg.html><span>Presto</span></a></li><li><a target=_blank href=https://docs.dremio.com/data-formats/apache-iceberg/><span>Dremio</span></a></li><li><a target=_blank href=https://docs.starrocks.com/en-us/main/using_starrocks/External_table#apache-iceberg-external-table><span>StarRocks</span></a></li><li><a target=_blank href=https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg.html><span>Amazon Athena</span></a></li><li><a target=_blank href=https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-iceberg-use-cluster.html><span>Amazon EMR</span></a></li><li><a target=_blank href=https://impala.apache.org/docs/build/html/topics/impala_iceberg.html><span>Impala</span></a></li><li><a target=_blank href=https://doris.apache.org/docs/dev/lakehouse/multi-catalog/iceberg><span>Doris</span></a></li><li><a class="chevron-toggle collapsed" data-toggle=collapse data-parent=full href=#Integrations><span>Integrations</span>
<i class="fa fa-chevron-right"></i>
<i class="fa fa-chevron-down"></i></a></li><div id=Integrations class=collapse><ul class=sub-menu><li><a href=../aws/>AWS</a></li><li><a href=../dell/>Dell</a></li><li><a href=../jdbc/>JDBC</a></li><li><a href=../nessie/>Nessie</a></li></ul></div><li><a class="chevron-toggle collapsed" data-toggle=collapse data-parent=full href=#API><span>API</span>
<i class="fa fa-chevron-right"></i>
<i class="fa fa-chevron-down"></i></a></li><div id=API class=collapse><ul class=sub-menu><li><a href=../java-api-quickstart/>Java Quickstart</a></li><li><a href=../api/>Java API</a></li><li><a href=../custom-catalog/>Java Custom Catalog</a></li></ul></div><li><a href=https://iceberg.apache.org/docs/1.2.0/../../javadoc/latest><span>Javadoc</span></a></li><li><a target=_blank href=https://py.iceberg.apache.org/><span>PyIceberg</span></a></li></div></div><div id=content class=markdown-body><div class=margin-for-toc><h1 id=flink>Flink</h1><p>Apache Iceberg supports both <a href=https://flink.apache.org/>Apache Flink</a>&rsquo;s DataStream API and Table API. See the <a href=https://iceberg.apache.org/multi-engine-support/#apache-flink>Multi-Engine Support#apache-flink</a> page for the integration of Apache Flink.</p><table><thead><tr><th>Feature support</th><th>Flink</th><th>Notes</th></tr></thead><tbody><tr><td><a href=#creating-catalogs-and-using-catalogs>SQL create catalog</a></td><td>‚úîÔ∏è</td><td></td></tr><tr><td><a href=#create-database>SQL create database</a></td><td>‚úîÔ∏è</td><td></td></tr><tr><td><a href=#create-table>SQL create table</a></td><td>‚úîÔ∏è</td><td></td></tr><tr><td><a href=#create-table-like>SQL create table like</a></td><td>‚úîÔ∏è</td><td></td></tr><tr><td><a href=#alter-table>SQL alter table</a></td><td>‚úîÔ∏è</td><td>Only support altering table properties, column and partition changes are not supported</td></tr><tr><td><a href=#drop-table>SQL drop_table</a></td><td>‚úîÔ∏è</td><td></td></tr><tr><td><a href=#querying-with-sql>SQL select</a></td><td>‚úîÔ∏è</td><td>Support both streaming and batch mode</td></tr><tr><td><a href=#insert-into>SQL insert into</a></td><td>‚úîÔ∏è Ô∏è</td><td>Support both streaming and batch mode</td></tr><tr><td><a href=#insert-overwrite>SQL insert overwrite</a></td><td>‚úîÔ∏è Ô∏è</td><td></td></tr><tr><td><a href=#reading-with-datastream>DataStream read</a></td><td>‚úîÔ∏è Ô∏è</td><td></td></tr><tr><td><a href=#appending-data>DataStream append</a></td><td>‚úîÔ∏è Ô∏è</td><td></td></tr><tr><td><a href=#overwrite-data>DataStream overwrite</a></td><td>‚úîÔ∏è Ô∏è</td><td></td></tr><tr><td><a href=#inspecting-tables>Metadata tables</a></td><td>Ô∏è</td><td>Support Java API but does not support Flink SQL</td></tr><tr><td><a href=#rewrite-files-action>Rewrite files action</a></td><td>‚úîÔ∏è Ô∏è</td><td></td></tr></tbody></table><h2 id=preparation-when-using-flink-sql-client>Preparation when using Flink SQL Client</h2><p>To create Iceberg table in Flink, it is recommended to use <a href=https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/sqlClient.html>Flink SQL Client</a> as it&rsquo;s easier for users to understand the concepts.</p><p>Download Flink from the <a href=https://flink.apache.org/downloads.html>Apache download page</a>. Iceberg uses Scala 2.12 when compiling the Apache <code>iceberg-flink-runtime</code> jar, so it&rsquo;s recommended to use Flink 1.16 bundled with Scala 2.12.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>FLINK_VERSION<span style=color:#f92672>=</span>1.16.1
</span></span><span style=display:flex><span>SCALA_VERSION<span style=color:#f92672>=</span>2.12
</span></span><span style=display:flex><span>APACHE_FLINK_URL<span style=color:#f92672>=</span>https://archive.apache.org/dist/flink/
</span></span><span style=display:flex><span>wget <span style=color:#e6db74>${</span>APACHE_FLINK_URL<span style=color:#e6db74>}</span>/flink-<span style=color:#e6db74>${</span>FLINK_VERSION<span style=color:#e6db74>}</span>/flink-<span style=color:#e6db74>${</span>FLINK_VERSION<span style=color:#e6db74>}</span>-bin-scala_<span style=color:#e6db74>${</span>SCALA_VERSION<span style=color:#e6db74>}</span>.tgz
</span></span><span style=display:flex><span>tar xzvf flink-<span style=color:#e6db74>${</span>FLINK_VERSION<span style=color:#e6db74>}</span>-bin-scala_<span style=color:#e6db74>${</span>SCALA_VERSION<span style=color:#e6db74>}</span>.tgz
</span></span></code></pre></div><p>Start a standalone Flink cluster within Hadoop environment:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># HADOOP_HOME is your hadoop root directory after unpack the binary package.</span>
</span></span><span style=display:flex><span>APACHE_HADOOP_URL<span style=color:#f92672>=</span>https://archive.apache.org/dist/hadoop/
</span></span><span style=display:flex><span>HADOOP_VERSION<span style=color:#f92672>=</span>2.8.5
</span></span><span style=display:flex><span>wget <span style=color:#e6db74>${</span>APACHE_HADOOP_URL<span style=color:#e6db74>}</span>/common/hadoop-<span style=color:#e6db74>${</span>HADOOP_VERSION<span style=color:#e6db74>}</span>/hadoop-<span style=color:#e6db74>${</span>HADOOP_VERSION<span style=color:#e6db74>}</span>.tar.gz
</span></span><span style=display:flex><span>tar xzvf hadoop-<span style=color:#e6db74>${</span>HADOOP_VERSION<span style=color:#e6db74>}</span>.tar.gz
</span></span><span style=display:flex><span>HADOOP_HOME<span style=color:#f92672>=</span><span style=color:#e6db74>`</span>pwd<span style=color:#e6db74>`</span>/hadoop-<span style=color:#e6db74>${</span>HADOOP_VERSION<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>export HADOOP_CLASSPATH<span style=color:#f92672>=</span><span style=color:#e6db74>`</span>$HADOOP_HOME/bin/hadoop classpath<span style=color:#e6db74>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Start the flink standalone cluster</span>
</span></span><span style=display:flex><span>./bin/start-cluster.sh
</span></span></code></pre></div><p>Start the Flink SQL client. There is a separate <code>flink-runtime</code> module in the Iceberg project to generate a bundled jar, which could be loaded by Flink SQL client directly. To build the <code>flink-runtime</code> bundled jar manually, build the <code>iceberg</code> project, and it will generate the jar under <code>&lt;iceberg-root-dir>/flink-runtime/build/libs</code>. Or download the <code>flink-runtime</code> jar from the <a href=https://repo.maven.apache.org/maven2/org/apache/iceberg/iceberg-flink-runtime-1.16/1.2.0/>Apache repository</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># HADOOP_HOME is your hadoop root directory after unpack the binary package.</span>
</span></span><span style=display:flex><span>export HADOOP_CLASSPATH<span style=color:#f92672>=</span><span style=color:#e6db74>`</span>$HADOOP_HOME/bin/hadoop classpath<span style=color:#e6db74>`</span>   
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>./bin/sql-client.sh embedded -j &lt;flink-runtime-directory&gt;/iceberg-flink-runtime-1.16-1.2.0.jar shell
</span></span></code></pre></div><p>By default, Iceberg ships with Hadoop jars for Hadoop catalog. To use Hive catalog, load the Hive jars when opening the Flink SQL client. Fortunately, Flink has provided a <a href=https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-2.3.9_2.12/1.16.1/flink-sql-connector-hive-2.3.9_2.12-1.16.1.jar>bundled hive jar</a> for the SQL client. An example on how to download the dependencies and get started:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># HADOOP_HOME is your hadoop root directory after unpack the binary package.</span>
</span></span><span style=display:flex><span>export HADOOP_CLASSPATH<span style=color:#f92672>=</span><span style=color:#e6db74>`</span>$HADOOP_HOME/bin/hadoop classpath<span style=color:#e6db74>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ICEBERG_VERSION<span style=color:#f92672>=</span>1.2.0
</span></span><span style=display:flex><span>MAVEN_URL<span style=color:#f92672>=</span>https://repo1.maven.org/maven2
</span></span><span style=display:flex><span>ICEBERG_MAVEN_URL<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>MAVEN_URL<span style=color:#e6db74>}</span>/org/apache/iceberg
</span></span><span style=display:flex><span>ICEBERG_PACKAGE<span style=color:#f92672>=</span>iceberg-flink-runtime
</span></span><span style=display:flex><span>wget <span style=color:#e6db74>${</span>ICEBERG_MAVEN_URL<span style=color:#e6db74>}</span>/<span style=color:#e6db74>${</span>ICEBERG_PACKAGE<span style=color:#e6db74>}</span>-<span style=color:#e6db74>${</span>FLINK_VERSION_MAJOR<span style=color:#e6db74>}</span>/<span style=color:#e6db74>${</span>ICEBERG_VERSION<span style=color:#e6db74>}</span>/<span style=color:#e6db74>${</span>ICEBERG_PACKAGE<span style=color:#e6db74>}</span>-<span style=color:#e6db74>${</span>FLINK_VERSION_MAJOR<span style=color:#e6db74>}</span>-<span style=color:#e6db74>${</span>ICEBERG_VERSION<span style=color:#e6db74>}</span>.jar -P lib/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>HIVE_VERSION<span style=color:#f92672>=</span>2.3.9
</span></span><span style=display:flex><span>SCALA_VERSION<span style=color:#f92672>=</span>2.12
</span></span><span style=display:flex><span>FLINK_VERSION<span style=color:#f92672>=</span>1.16.1
</span></span><span style=display:flex><span>FLINK_CONNECTOR_URL<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>MAVEN_URL<span style=color:#e6db74>}</span>/org/apache/flink
</span></span><span style=display:flex><span>FLINK_CONNECTOR_PACKAGE<span style=color:#f92672>=</span>flink-sql-connector-hive
</span></span><span style=display:flex><span>wget <span style=color:#e6db74>${</span>FLINK_CONNECTOR_URL<span style=color:#e6db74>}</span>/<span style=color:#e6db74>${</span>FLINK_CONNECTOR_PACKAGE<span style=color:#e6db74>}</span>-<span style=color:#e6db74>${</span>HIVE_VERSION<span style=color:#e6db74>}</span>_<span style=color:#e6db74>${</span>SCALA_VERSION<span style=color:#e6db74>}</span>/<span style=color:#e6db74>${</span>FLINK_VERSION<span style=color:#e6db74>}</span>/<span style=color:#e6db74>${</span>FLINK_CONNECTOR_PACKAGE<span style=color:#e6db74>}</span>-<span style=color:#e6db74>${</span>HIVE_VERSION<span style=color:#e6db74>}</span>_<span style=color:#e6db74>${</span>SCALA_VERSION<span style=color:#e6db74>}</span>-<span style=color:#e6db74>${</span>FLINK_VERSION<span style=color:#e6db74>}</span>.jar
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>./bin/sql-client.sh embedded shell
</span></span></code></pre></div><h2 id=flinks-python-api>Flink&rsquo;s Python API</h2><div class=info>PyFlink 1.6.1 <a href=https://issues.apache.org/jira/browse/FLINK-28786>does not work on OSX with a M1 cpu</a></div><p>Install the Apache Flink dependency using <code>pip</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pip install apache<span style=color:#f92672>-</span>flink<span style=color:#f92672>==</span><span style=color:#ae81ff>1.16.1</span>
</span></span></code></pre></div><p>Provide a <code>file://</code> path to the <code>iceberg-flink-runtime</code> jar, which can be obtained by building the project and looking at <code>&lt;iceberg-root-dir>/flink-runtime/build/libs</code>, or downloading it from the <a href=https://repo.maven.apache.org/maven2/org/apache/iceberg/iceberg-flink-runtime/>Apache official repository</a>. Third-party jars can be added to <code>pyflink</code> via:</p><ul><li><code>env.add_jars("file:///my/jar/path/connector.jar")</code></li><li><code>table_env.get_config().get_configuration().set_string("pipeline.jars", "file:///my/jar/path/connector.jar")</code></li></ul><p>This is also mentioned in the official <a href=https://ci.apache.org/projects/flink/flink-docs-release-1.16/docs/dev/python/dependency_management/>docs</a>. The example below uses <code>env.add_jars(..)</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyflink.datastream <span style=color:#f92672>import</span> StreamExecutionEnvironment
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>env <span style=color:#f92672>=</span> StreamExecutionEnvironment<span style=color:#f92672>.</span>get_execution_environment()
</span></span><span style=display:flex><span>iceberg_flink_runtime_jar <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(os<span style=color:#f92672>.</span>getcwd(), <span style=color:#e6db74>&#34;iceberg-flink-runtime-1.16-1.2.0.jar&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>env<span style=color:#f92672>.</span>add_jars(<span style=color:#e6db74>&#34;file://</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(iceberg_flink_runtime_jar))
</span></span></code></pre></div><p>Next, create a <code>StreamTableEnvironment</code> and execute Flink SQL statements. The below example shows how to create a custom catalog via the Python Table API:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyflink.table <span style=color:#f92672>import</span> StreamTableEnvironment
</span></span><span style=display:flex><span>table_env <span style=color:#f92672>=</span> StreamTableEnvironment<span style=color:#f92672>.</span>create(env)
</span></span><span style=display:flex><span>table_env<span style=color:#f92672>.</span>execute_sql(<span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>CREATE CATALOG my_catalog WITH (
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;type&#39;=&#39;iceberg&#39;, 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;catalog-impl&#39;=&#39;com.my.custom.CatalogImpl&#39;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;my-additional-catalog-config&#39;=&#39;my-value&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>)
</span></span></code></pre></div><p>Run a query:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>(table_env
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>sql_query(<span style=color:#e6db74>&#34;SELECT PULocationID, DOLocationID, passenger_count FROM my_catalog.nyc.taxis LIMIT 5&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>execute()
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>print()) 
</span></span></code></pre></div><pre tabindex=0><code>+----+----------------------+----------------------+--------------------------------+
| op |         PULocationID |         DOLocationID |                passenger_count |
+----+----------------------+----------------------+--------------------------------+
| +I |                  249 |                   48 |                            1.0 |
| +I |                  132 |                  233 |                            1.0 |
| +I |                  164 |                  107 |                            1.0 |
| +I |                   90 |                  229 |                            1.0 |
| +I |                  137 |                  249 |                            1.0 |
+----+----------------------+----------------------+--------------------------------+
5 rows in set
</code></pre><p>For more details, please refer to the <a href=https://ci.apache.org/projects/flink/flink-docs-release-1.16/docs/dev/python/table/intro_to_table_api/>Python Table API</a>.</p><h2 id=creating-catalogs-and-using-catalogs>Creating catalogs and using catalogs.</h2><p>Flink support to create catalogs by using Flink SQL.</p><h3 id=catalog-configuration>Catalog Configuration</h3><p>A catalog is created and named by executing the following query (replace <code>&lt;catalog_name></code> with your catalog name and
<code>&lt;config_key></code>=<code>&lt;config_value></code> with catalog implementation config):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>CATALOG</span> <span style=color:#f92672>&lt;</span><span style=color:#66d9ef>catalog_name</span><span style=color:#f92672>&gt;</span> <span style=color:#66d9ef>WITH</span> (
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;type&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;iceberg&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>`&lt;</span>config_key<span style=color:#f92672>&gt;`=`&lt;</span>config_value<span style=color:#f92672>&gt;`</span>
</span></span><span style=display:flex><span>); 
</span></span></code></pre></div><p>The following properties can be set globally and are not limited to a specific catalog implementation:</p><ul><li><code>type</code>: Must be <code>iceberg</code>. (required)</li><li><code>catalog-type</code>: <code>hive</code>, <code>hadoop</code> or <code>rest</code> for built-in catalogs, or left unset for custom catalog implementations using catalog-impl. (Optional)</li><li><code>catalog-impl</code>: The fully-qualified class name of a custom catalog implementation. Must be set if <code>catalog-type</code> is unset. (Optional)</li><li><code>property-version</code>: Version number to describe the property version. This property can be used for backwards compatibility in case the property format changes. The current property version is <code>1</code>. (Optional)</li><li><code>cache-enabled</code>: Whether to enable catalog cache, default value is <code>true</code>. (Optional)</li><li><code>cache.expiration-interval-ms</code>: How long catalog entries are locally cached, in milliseconds; negative values like <code>-1</code> will disable expiration, value 0 is not allowed to set. default value is <code>-1</code>. (Optional)</li></ul><h3 id=hive-catalog>Hive catalog</h3><p>This creates an Iceberg catalog named <code>hive_catalog</code> that can be configured using <code>'catalog-type'='hive'</code>, which loads tables from Hive metastore:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>CATALOG</span> hive_catalog <span style=color:#66d9ef>WITH</span> (
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;type&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;iceberg&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;catalog-type&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;hive&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;uri&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;thrift://localhost:9083&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;clients&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;5&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;property-version&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;1&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;warehouse&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;hdfs://nn:8020/warehouse/path&#39;</span>
</span></span><span style=display:flex><span>);
</span></span></code></pre></div><p>The following properties can be set if using the Hive catalog:</p><ul><li><code>uri</code>: The Hive metastore&rsquo;s thrift URI. (Required)</li><li><code>clients</code>: The Hive metastore client pool size, default value is 2. (Optional)</li><li><code>warehouse</code>: The Hive warehouse location, users should specify this path if neither set the <code>hive-conf-dir</code> to specify a location containing a <code>hive-site.xml</code> configuration file nor add a correct <code>hive-site.xml</code> to classpath.</li><li><code>hive-conf-dir</code>: Path to a directory containing a <code>hive-site.xml</code> configuration file which will be used to provide custom Hive configuration values. The value of <code>hive.metastore.warehouse.dir</code> from <code>&lt;hive-conf-dir>/hive-site.xml</code> (or hive configure file from classpath) will be overwritten with the <code>warehouse</code> value if setting both <code>hive-conf-dir</code> and <code>warehouse</code> when creating iceberg catalog.</li><li><code>hadoop-conf-dir</code>: Path to a directory containing <code>core-site.xml</code> and <code>hdfs-site.xml</code> configuration files which will be used to provide custom Hadoop configuration values.</li></ul><h3 id=hadoop-catalog>Hadoop catalog</h3><p>Iceberg also supports a directory-based catalog in HDFS that can be configured using <code>'catalog-type'='hadoop'</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>CATALOG</span> hadoop_catalog <span style=color:#66d9ef>WITH</span> (
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;type&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;iceberg&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;catalog-type&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;hadoop&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;warehouse&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;hdfs://nn:8020/warehouse/path&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;property-version&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;1&#39;</span>
</span></span><span style=display:flex><span>);
</span></span></code></pre></div><p>The following properties can be set if using the Hadoop catalog:</p><ul><li><code>warehouse</code>: The HDFS directory to store metadata files and data files. (Required)</li></ul><p>Execute the sql command <code>USE CATALOG hadoop_catalog</code> to set the current catalog.</p><h3 id=rest-catalog>REST catalog</h3><p>This creates an iceberg catalog named <code>rest_catalog</code> that can be configured using <code>'catalog-type'='rest'</code>, which loads tables from a REST catalog:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>CATALOG</span> rest_catalog <span style=color:#66d9ef>WITH</span> (
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;type&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;iceberg&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;catalog-type&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;rest&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;uri&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;https://localhost/&#39;</span>
</span></span><span style=display:flex><span>);
</span></span></code></pre></div><p>The following properties can be set if using the REST catalog:</p><ul><li><code>uri</code>: The URL to the REST Catalog (Required)</li><li><code>credential</code>: A credential to exchange for a token in the OAuth2 client credentials flow (Optional)</li><li><code>token</code>: A token which will be used to interact with the server (Optional)</li></ul><h3 id=custom-catalog>Custom catalog</h3><p>Flink also supports loading a custom Iceberg <code>Catalog</code> implementation by specifying the <code>catalog-impl</code> property:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>CATALOG</span> my_catalog <span style=color:#66d9ef>WITH</span> (
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;type&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;iceberg&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;catalog-impl&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;com.my.custom.CatalogImpl&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;my-additional-catalog-config&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;my-value&#39;</span>
</span></span><span style=display:flex><span>);
</span></span></code></pre></div><h3 id=create-through-yaml-config>Create through YAML config</h3><p>Catalogs can be registered in <code>sql-client-defaults.yaml</code> before starting the SQL client.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>catalogs</span>: 
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>my_catalog</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>type</span>: <span style=color:#ae81ff>iceberg</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>catalog-type</span>: <span style=color:#ae81ff>hadoop</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>warehouse</span>: <span style=color:#ae81ff>hdfs://nn:8020/warehouse/path</span>
</span></span></code></pre></div><h3 id=create-through-sql-files>Create through SQL Files</h3><p>The Flink SQL Client supports the <code>-i</code> startup option to execute an initialization SQL file to set up environment when starting up the SQL Client.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#75715e>-- define available catalogs
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>CATALOG</span> hive_catalog <span style=color:#66d9ef>WITH</span> (
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;type&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;iceberg&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;catalog-type&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;hive&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;uri&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;thrift://localhost:9083&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;warehouse&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;hdfs://nn:8020/warehouse/path&#39;</span>
</span></span><span style=display:flex><span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>USE <span style=color:#66d9ef>CATALOG</span> hive_catalog;
</span></span></code></pre></div><p>Using <code>-i &lt;init.sql></code> option to initialize SQL Client session:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>/path/to/bin/sql-client.sh -i /path/to/init.sql
</span></span></code></pre></div><h2 id=ddl-commands>DDL commands</h2><h3 id=create-database><code>CREATE DATABASE</code></h3><p>By default, Iceberg will use the <code>default</code> database in Flink. Using the following example to create a separate database in order to avoid creating tables under the <code>default</code> database:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>DATABASE</span> iceberg_db;
</span></span><span style=display:flex><span>USE iceberg_db;
</span></span></code></pre></div><h3 id=create-table><code>CREATE TABLE</code></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>TABLE</span> <span style=color:#f92672>`</span>hive_catalog<span style=color:#f92672>`</span>.<span style=color:#f92672>`</span><span style=color:#66d9ef>default</span><span style=color:#f92672>`</span>.<span style=color:#f92672>`</span>sample<span style=color:#f92672>`</span> (
</span></span><span style=display:flex><span>    id BIGINT <span style=color:#66d9ef>COMMENT</span> <span style=color:#e6db74>&#39;unique id&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>data</span> STRING
</span></span><span style=display:flex><span>);
</span></span></code></pre></div><p>Table create commands support the commonly used <a href=https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/>Flink create clauses</a> including:</p><ul><li><code>PARTITION BY (column1, column2, ...)</code> to configure partitioning, Flink does not yet support hidden partitioning.</li><li><code>COMMENT 'table document'</code> to set a table description.</li><li><code>WITH ('key'='value', ...)</code> to set <a href=../configuration>table configuration</a> which will be stored in Iceberg table properties.</li></ul><p>Currently, it does not support computed column, primary key and watermark definition etc.</p><h3 id=partitioned-by><code>PARTITIONED BY</code></h3><p>To create a partition table, use <code>PARTITIONED BY</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>TABLE</span> <span style=color:#f92672>`</span>hive_catalog<span style=color:#f92672>`</span>.<span style=color:#f92672>`</span><span style=color:#66d9ef>default</span><span style=color:#f92672>`</span>.<span style=color:#f92672>`</span>sample<span style=color:#f92672>`</span> (
</span></span><span style=display:flex><span>    id BIGINT <span style=color:#66d9ef>COMMENT</span> <span style=color:#e6db74>&#39;unique id&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>data</span> STRING
</span></span><span style=display:flex><span>) PARTITIONED <span style=color:#66d9ef>BY</span> (<span style=color:#66d9ef>data</span>);
</span></span></code></pre></div><p>Iceberg support hidden partition but Flink don&rsquo;t support partitioning by a function on columns, so there is no way to support hidden partition in Flink DDL.</p><h3 id=create-table-like><code>CREATE TABLE LIKE</code></h3><p>To create a table with the same schema, partitioning, and table properties as another table, use <code>CREATE TABLE LIKE</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>TABLE</span> <span style=color:#f92672>`</span>hive_catalog<span style=color:#f92672>`</span>.<span style=color:#f92672>`</span><span style=color:#66d9ef>default</span><span style=color:#f92672>`</span>.<span style=color:#f92672>`</span>sample<span style=color:#f92672>`</span> (
</span></span><span style=display:flex><span>    id BIGINT <span style=color:#66d9ef>COMMENT</span> <span style=color:#e6db74>&#39;unique id&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>data</span> STRING
</span></span><span style=display:flex><span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>TABLE</span>  <span style=color:#f92672>`</span>hive_catalog<span style=color:#f92672>`</span>.<span style=color:#f92672>`</span><span style=color:#66d9ef>default</span><span style=color:#f92672>`</span>.<span style=color:#f92672>`</span>sample_like<span style=color:#f92672>`</span> <span style=color:#66d9ef>LIKE</span> <span style=color:#f92672>`</span>hive_catalog<span style=color:#f92672>`</span>.<span style=color:#f92672>`</span><span style=color:#66d9ef>default</span><span style=color:#f92672>`</span>.<span style=color:#f92672>`</span>sample<span style=color:#f92672>`</span>;
</span></span></code></pre></div><p>For more details, refer to the <a href=https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/table/sql/create/>Flink <code>CREATE TABLE</code> documentation</a>.</p><h3 id=alter-table><code>ALTER TABLE</code></h3><p>Iceberg only support altering table properties:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>ALTER</span> <span style=color:#66d9ef>TABLE</span> <span style=color:#f92672>`</span>hive_catalog<span style=color:#f92672>`</span>.<span style=color:#f92672>`</span><span style=color:#66d9ef>default</span><span style=color:#f92672>`</span>.<span style=color:#f92672>`</span>sample<span style=color:#f92672>`</span> <span style=color:#66d9ef>SET</span> (<span style=color:#e6db74>&#39;write.format.default&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;avro&#39;</span>)
</span></span></code></pre></div><h3 id=alter-table--rename-to><code>ALTER TABLE .. RENAME TO</code></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>ALTER</span> <span style=color:#66d9ef>TABLE</span> <span style=color:#f92672>`</span>hive_catalog<span style=color:#f92672>`</span>.<span style=color:#f92672>`</span><span style=color:#66d9ef>default</span><span style=color:#f92672>`</span>.<span style=color:#f92672>`</span>sample<span style=color:#f92672>`</span> <span style=color:#66d9ef>RENAME</span> <span style=color:#66d9ef>TO</span> <span style=color:#f92672>`</span>hive_catalog<span style=color:#f92672>`</span>.<span style=color:#f92672>`</span><span style=color:#66d9ef>default</span><span style=color:#f92672>`</span>.<span style=color:#f92672>`</span>new_sample<span style=color:#f92672>`</span>;
</span></span></code></pre></div><h3 id=drop-table><code>DROP TABLE</code></h3><p>To delete a table, run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>DROP</span> <span style=color:#66d9ef>TABLE</span> <span style=color:#f92672>`</span>hive_catalog<span style=color:#f92672>`</span>.<span style=color:#f92672>`</span><span style=color:#66d9ef>default</span><span style=color:#f92672>`</span>.<span style=color:#f92672>`</span>sample<span style=color:#f92672>`</span>;
</span></span></code></pre></div><h2 id=querying-with-sql>Querying with SQL</h2><p>Iceberg support both streaming and batch read in Flink. Execute the following sql command to switch execution mode from <code>streaming</code> to <code>batch</code>, and vice versa:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#75715e>-- Execute the flink job in streaming mode for current session context
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>SET</span> execution.runtime<span style=color:#f92672>-</span><span style=color:#66d9ef>mode</span> <span style=color:#f92672>=</span> streaming;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>-- Execute the flink job in batch mode for current session context
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>SET</span> execution.runtime<span style=color:#f92672>-</span><span style=color:#66d9ef>mode</span> <span style=color:#f92672>=</span> batch;
</span></span></code></pre></div><h3 id=flink-batch-read>Flink batch read</h3><p>Submit a Flink <strong>batch</strong> job using the following sentences:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#75715e>-- Execute the flink job in batch mode for current session context
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>SET</span> execution.runtime<span style=color:#f92672>-</span><span style=color:#66d9ef>mode</span> <span style=color:#f92672>=</span> batch;
</span></span><span style=display:flex><span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> sample;
</span></span></code></pre></div><h3 id=flink-streaming-read>Flink streaming read</h3><p>Iceberg supports processing incremental data in flink streaming jobs which starts from a historical snapshot-id:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#75715e>-- Submit the flink job in streaming mode for current session.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>SET</span> execution.runtime<span style=color:#f92672>-</span><span style=color:#66d9ef>mode</span> <span style=color:#f92672>=</span> streaming;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>-- Enable this switch because streaming read SQL will provide few job options in flink SQL hint options.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>SET</span> <span style=color:#66d9ef>table</span>.<span style=color:#66d9ef>dynamic</span><span style=color:#f92672>-</span><span style=color:#66d9ef>table</span><span style=color:#f92672>-</span><span style=color:#66d9ef>options</span>.enabled<span style=color:#f92672>=</span><span style=color:#66d9ef>true</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>-- Read all the records from the iceberg current snapshot, and then read incremental data starting from that snapshot.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> sample <span style=color:#75715e>/*+ OPTIONS(&#39;streaming&#39;=&#39;true&#39;, &#39;monitor-interval&#39;=&#39;1s&#39;)*/</span> ;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>-- Read all incremental data starting from the snapshot-id &#39;3821550127947089987&#39; (records from this snapshot will be excluded).
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> sample <span style=color:#75715e>/*+ OPTIONS(&#39;streaming&#39;=&#39;true&#39;, &#39;monitor-interval&#39;=&#39;1s&#39;, &#39;start-snapshot-id&#39;=&#39;3821550127947089987&#39;)*/</span> ;
</span></span></code></pre></div><p>There are some options that could be set in Flink SQL hint options for streaming job, see <a href=#Read-options>read options</a> for details.</p><h3 id=flip-27-source-for-sql>FLIP-27 source for SQL</h3><p>Here are the SQL settings for the <a href=https://cwiki.apache.org/confluence/display/FLINK/FLIP-27%3A+Refactor+Source+Interface>FLIP-27</a> source. All other SQL settings and options documented above are applicable to the FLIP-27 source.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#75715e>-- Opt in the FLIP-27 source. Default is false.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>SET</span> <span style=color:#66d9ef>table</span>.<span style=color:#66d9ef>exec</span>.iceberg.use<span style=color:#f92672>-</span>flip27<span style=color:#f92672>-</span><span style=color:#66d9ef>source</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>true</span>;
</span></span></code></pre></div><h2 id=writing-with-sql>Writing with SQL</h2><p>Iceberg support both <code>INSERT INTO</code> and <code>INSERT OVERWRITE</code>.</p><h3 id=insert-into><code>INSERT INTO</code></h3><p>To append new data to a table with a Flink streaming job, use <code>INSERT INTO</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>INSERT</span> <span style=color:#66d9ef>INTO</span> <span style=color:#f92672>`</span>hive_catalog<span style=color:#f92672>`</span>.<span style=color:#f92672>`</span><span style=color:#66d9ef>default</span><span style=color:#f92672>`</span>.<span style=color:#f92672>`</span>sample<span style=color:#f92672>`</span> <span style=color:#66d9ef>VALUES</span> (<span style=color:#ae81ff>1</span>, <span style=color:#e6db74>&#39;a&#39;</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>INSERT</span> <span style=color:#66d9ef>INTO</span> <span style=color:#f92672>`</span>hive_catalog<span style=color:#f92672>`</span>.<span style=color:#f92672>`</span><span style=color:#66d9ef>default</span><span style=color:#f92672>`</span>.<span style=color:#f92672>`</span>sample<span style=color:#f92672>`</span> <span style=color:#66d9ef>SELECT</span> id, <span style=color:#66d9ef>data</span> <span style=color:#66d9ef>from</span> other_kafka_table;
</span></span></code></pre></div><h3 id=insert-overwrite><code>INSERT OVERWRITE</code></h3><p>To replace data in the table with the result of a query, use <code>INSERT OVERWRITE</code> in batch job (flink streaming job does not support <code>INSERT OVERWRITE</code>). Overwrites are atomic operations for Iceberg tables.</p><p>Partitions that have rows produced by the SELECT query will be replaced, for example:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>INSERT</span> OVERWRITE sample <span style=color:#66d9ef>VALUES</span> (<span style=color:#ae81ff>1</span>, <span style=color:#e6db74>&#39;a&#39;</span>);
</span></span></code></pre></div><p>Iceberg also support overwriting given partitions by the <code>select</code> values:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>INSERT</span> OVERWRITE <span style=color:#f92672>`</span>hive_catalog<span style=color:#f92672>`</span>.<span style=color:#f92672>`</span><span style=color:#66d9ef>default</span><span style=color:#f92672>`</span>.<span style=color:#f92672>`</span>sample<span style=color:#f92672>`</span> PARTITION(<span style=color:#66d9ef>data</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;a&#39;</span>) <span style=color:#66d9ef>SELECT</span> <span style=color:#ae81ff>6</span>;
</span></span></code></pre></div><p>For a partitioned iceberg table, when all the partition columns are set a value in <code>PARTITION</code> clause, it is inserting into a static partition, otherwise if partial partition columns (prefix part of all partition columns) are set a value in <code>PARTITION</code> clause, it is writing the query result into a dynamic partition.
For an unpartitioned iceberg table, its data will be completely overwritten by <code>INSERT OVERWRITE</code>.</p><h3 id=upsert><code>UPSERT</code></h3><p>Iceberg supports <code>UPSERT</code> based on the primary key when writing data into v2 table format. There are two ways to enable upsert.</p><ol><li>Enable the <code>UPSERT</code> mode as table-level property <code>write.upsert.enabled</code>. Here is an example SQL statement to set the table property when creating a table. It would be applied for all write paths to this table (batch or streaming) unless overwritten by write options as described later.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>TABLE</span> <span style=color:#f92672>`</span>hive_catalog<span style=color:#f92672>`</span>.<span style=color:#f92672>`</span><span style=color:#66d9ef>default</span><span style=color:#f92672>`</span>.<span style=color:#f92672>`</span>sample<span style=color:#f92672>`</span> (
</span></span><span style=display:flex><span>  <span style=color:#f92672>`</span>id<span style=color:#f92672>`</span>  INT <span style=color:#66d9ef>UNIQUE</span> <span style=color:#66d9ef>COMMENT</span> <span style=color:#e6db74>&#39;unique id&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>`</span><span style=color:#66d9ef>data</span><span style=color:#f92672>`</span> STRING <span style=color:#66d9ef>NOT</span> <span style=color:#66d9ef>NULL</span>,
</span></span><span style=display:flex><span> <span style=color:#66d9ef>PRIMARY</span> <span style=color:#66d9ef>KEY</span>(<span style=color:#f92672>`</span>id<span style=color:#f92672>`</span>) <span style=color:#66d9ef>NOT</span> ENFORCED
</span></span><span style=display:flex><span>) <span style=color:#66d9ef>with</span> (<span style=color:#e6db74>&#39;format-version&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;2&#39;</span>, <span style=color:#e6db74>&#39;write.upsert.enabled&#39;</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#39;true&#39;</span>);
</span></span></code></pre></div><ol start=2><li>Enabling <code>UPSERT</code> mode using <code>upsert-enabled</code> in the <a href=#Write-options>write options</a> provides more flexibility than a table level config. Note that you still need to use v2 table format and specify the primary key when creating the table.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>INSERT</span> <span style=color:#66d9ef>INTO</span> tableName <span style=color:#75715e>/*+ OPTIONS(&#39;upsert-enabled&#39;=&#39;true&#39;) */</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><div class=info>OVERWRITE and UPSERT can&rsquo;t be set together. In UPSERT mode, if the table is partitioned, the partition fields should be included in equality fields.</div><h2 id=reading-with-datastream>Reading with DataStream</h2><p>Iceberg support streaming or batch read in Java API now.</p><h3 id=batch-read>Batch Read</h3><p>This example will read all records from iceberg table and then print to the stdout console in flink batch job:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span>StreamExecutionEnvironment env <span style=color:#f92672>=</span> StreamExecutionEnvironment<span style=color:#f92672>.</span><span style=color:#a6e22e>createLocalEnvironment</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>TableLoader tableLoader <span style=color:#f92672>=</span> TableLoader<span style=color:#f92672>.</span><span style=color:#a6e22e>fromHadoopTable</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;hdfs://nn:8020/warehouse/path&#34;</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>DataStream<span style=color:#f92672>&lt;</span>RowData<span style=color:#f92672>&gt;</span> batch <span style=color:#f92672>=</span> FlinkSource<span style=color:#f92672>.</span><span style=color:#a6e22e>forRowData</span><span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span><span style=color:#a6e22e>env</span><span style=color:#f92672>(</span>env<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span><span style=color:#a6e22e>tableLoader</span><span style=color:#f92672>(</span>tableLoader<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span><span style=color:#a6e22e>streaming</span><span style=color:#f92672>(</span><span style=color:#66d9ef>false</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span><span style=color:#a6e22e>build</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Print all records to stdout.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>batch<span style=color:#f92672>.</span><span style=color:#a6e22e>print</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Submit and execute this batch read job.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>env<span style=color:#f92672>.</span><span style=color:#a6e22e>execute</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Test Iceberg Batch Read&#34;</span><span style=color:#f92672>);</span>
</span></span></code></pre></div><h3 id=streaming-read>Streaming read</h3><p>This example will read incremental records which start from snapshot-id &lsquo;3821550127947089987&rsquo; and print to stdout console in flink streaming job:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span>StreamExecutionEnvironment env <span style=color:#f92672>=</span> StreamExecutionEnvironment<span style=color:#f92672>.</span><span style=color:#a6e22e>createLocalEnvironment</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>TableLoader tableLoader <span style=color:#f92672>=</span> TableLoader<span style=color:#f92672>.</span><span style=color:#a6e22e>fromHadoopTable</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;hdfs://nn:8020/warehouse/path&#34;</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>DataStream<span style=color:#f92672>&lt;</span>RowData<span style=color:#f92672>&gt;</span> stream <span style=color:#f92672>=</span> FlinkSource<span style=color:#f92672>.</span><span style=color:#a6e22e>forRowData</span><span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span><span style=color:#a6e22e>env</span><span style=color:#f92672>(</span>env<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span><span style=color:#a6e22e>tableLoader</span><span style=color:#f92672>(</span>tableLoader<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span><span style=color:#a6e22e>streaming</span><span style=color:#f92672>(</span><span style=color:#66d9ef>true</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span><span style=color:#a6e22e>startSnapshotId</span><span style=color:#f92672>(</span><span style=color:#ae81ff>3821550127947089987L</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span><span style=color:#a6e22e>build</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Print all records to stdout.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>stream<span style=color:#f92672>.</span><span style=color:#a6e22e>print</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Submit and execute this streaming read job.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>env<span style=color:#f92672>.</span><span style=color:#a6e22e>execute</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Test Iceberg Streaming Read&#34;</span><span style=color:#f92672>);</span>
</span></span></code></pre></div><p>There are other options that can be set, please see the <a href=../../../javadoc/1.2.0/org/apache/iceberg/flink/source/FlinkSource.html>FlinkSource#Builder</a>.</p><h2 id=reading-with-datastream-flip-27-source>Reading with DataStream (FLIP-27 source)</h2><p><a href=https://cwiki.apache.org/confluence/display/FLINK/FLIP-27%3A+Refactor+Source+Interface>FLIP-27 source interface</a>
was introduced in Flink 1.12. It aims to solve several shortcomings of the old <code>SourceFunction</code>
streaming source interface. It also unifies the source interfaces for both batch and streaming executions.
Most source connectors (like Kafka, file) in Flink repo have migrated to the FLIP-27 interface.
Flink is planning to deprecate the old <code>SourceFunction</code> interface in the near future.</p><p>A FLIP-27 based Flink <code>IcebergSource</code> is added in <code>iceberg-flink</code> module. The FLIP-27 <code>IcebergSource</code> is currently an experimental feature.</p><h3 id=batch-read-1>Batch Read</h3><p>This example will read all records from iceberg table and then print to the stdout console in flink batch job:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span>StreamExecutionEnvironment env <span style=color:#f92672>=</span> StreamExecutionEnvironment<span style=color:#f92672>.</span><span style=color:#a6e22e>createLocalEnvironment</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>TableLoader tableLoader <span style=color:#f92672>=</span> TableLoader<span style=color:#f92672>.</span><span style=color:#a6e22e>fromHadoopTable</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;hdfs://nn:8020/warehouse/path&#34;</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>IcebergSource<span style=color:#f92672>&lt;</span>RowData<span style=color:#f92672>&gt;</span> source <span style=color:#f92672>=</span> IcebergSource<span style=color:#f92672>.</span><span style=color:#a6e22e>forRowData</span><span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>tableLoader</span><span style=color:#f92672>(</span>tableLoader<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>assignerFactory</span><span style=color:#f92672>(</span><span style=color:#66d9ef>new</span> SimpleSplitAssignerFactory<span style=color:#f92672>())</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>build</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>DataStream<span style=color:#f92672>&lt;</span>RowData<span style=color:#f92672>&gt;</span> batch <span style=color:#f92672>=</span> env<span style=color:#f92672>.</span><span style=color:#a6e22e>fromSource</span><span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>    source<span style=color:#f92672>,</span>
</span></span><span style=display:flex><span>    WatermarkStrategy<span style=color:#f92672>.</span><span style=color:#a6e22e>noWatermarks</span><span style=color:#f92672>(),</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;My Iceberg Source&#34;</span><span style=color:#f92672>,</span>
</span></span><span style=display:flex><span>    TypeInformation<span style=color:#f92672>.</span><span style=color:#a6e22e>of</span><span style=color:#f92672>(</span>RowData<span style=color:#f92672>.</span><span style=color:#a6e22e>class</span><span style=color:#f92672>));</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Print all records to stdout.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>batch<span style=color:#f92672>.</span><span style=color:#a6e22e>print</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Submit and execute this batch read job.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>env<span style=color:#f92672>.</span><span style=color:#a6e22e>execute</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Test Iceberg Batch Read&#34;</span><span style=color:#f92672>);</span>
</span></span></code></pre></div><h3 id=streaming-read-1>Streaming read</h3><p>This example will start the streaming read from the latest table snapshot (inclusive).
Every 60s, it polls Iceberg table to discover new append-only snapshots.
CDC read is not supported yet.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span>StreamExecutionEnvironment env <span style=color:#f92672>=</span> StreamExecutionEnvironment<span style=color:#f92672>.</span><span style=color:#a6e22e>createLocalEnvironment</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>TableLoader tableLoader <span style=color:#f92672>=</span> TableLoader<span style=color:#f92672>.</span><span style=color:#a6e22e>fromHadoopTable</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;hdfs://nn:8020/warehouse/path&#34;</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>IcebergSource source <span style=color:#f92672>=</span> IcebergSource<span style=color:#f92672>.</span><span style=color:#a6e22e>forRowData</span><span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>tableLoader</span><span style=color:#f92672>(</span>tableLoader<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>assignerFactory</span><span style=color:#f92672>(</span><span style=color:#66d9ef>new</span> SimpleSplitAssignerFactory<span style=color:#f92672>())</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>streaming</span><span style=color:#f92672>(</span><span style=color:#66d9ef>true</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>streamingStartingStrategy</span><span style=color:#f92672>(</span>StreamingStartingStrategy<span style=color:#f92672>.</span><span style=color:#a6e22e>INCREMENTAL_FROM_LATEST_SNAPSHOT</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>monitorInterval</span><span style=color:#f92672>(</span>Duration<span style=color:#f92672>.</span><span style=color:#a6e22e>ofSeconds</span><span style=color:#f92672>(</span><span style=color:#ae81ff>60</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>build</span><span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>DataStream<span style=color:#f92672>&lt;</span>RowData<span style=color:#f92672>&gt;</span> stream <span style=color:#f92672>=</span> env<span style=color:#f92672>.</span><span style=color:#a6e22e>fromSource</span><span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>    source<span style=color:#f92672>,</span>
</span></span><span style=display:flex><span>    WatermarkStrategy<span style=color:#f92672>.</span><span style=color:#a6e22e>noWatermarks</span><span style=color:#f92672>(),</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;My Iceberg Source&#34;</span><span style=color:#f92672>,</span>
</span></span><span style=display:flex><span>    TypeInformation<span style=color:#f92672>.</span><span style=color:#a6e22e>of</span><span style=color:#f92672>(</span>RowData<span style=color:#f92672>.</span><span style=color:#a6e22e>class</span><span style=color:#f92672>));</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Print all records to stdout.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>stream<span style=color:#f92672>.</span><span style=color:#a6e22e>print</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Submit and execute this streaming read job.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>env<span style=color:#f92672>.</span><span style=color:#a6e22e>execute</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Test Iceberg Streaming Read&#34;</span><span style=color:#f92672>);</span>
</span></span></code></pre></div><p>There are other options that could be set by Java API, please see the
<a href=../../../javadoc/1.2.0/org/apache/iceberg/flink/source/IcebergSource.html>IcebergSource#Builder</a>.</p><h3 id=read-as-avro-genericrecord>Read as Avro GenericRecord</h3><p>FLIP-27 Iceberg source provides <code>AvroGenericRecordReaderFunction</code> that converts
Flink <code>RowData</code> Avro <code>GenericRecord</code>. You can use the convert to read from
Iceberg table as Avro GenericRecord DataStream.</p><p>Please make sure <code>flink-avro</code> jar is included in the classpath.
Also <code>iceberg-flink-runtime</code> shaded bundle jar can&rsquo;t be used
because the runtime jar shades the avro package.
Please use non-shaded <code>iceberg-flink</code> jar instead.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span>TableLoader tableLoader <span style=color:#f92672>=</span> <span style=color:#f92672>...;</span>
</span></span><span style=display:flex><span>Table table<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>try</span> <span style=color:#f92672>(</span>TableLoader loader <span style=color:#f92672>=</span> tableLoader<span style=color:#f92672>)</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    loader<span style=color:#f92672>.</span><span style=color:#a6e22e>open</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>    table <span style=color:#f92672>=</span> loader<span style=color:#f92672>.</span><span style=color:#a6e22e>loadTable</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>AvroGenericRecordReaderFunction readerFunction <span style=color:#f92672>=</span> AvroGenericRecordReaderFunction<span style=color:#f92672>.</span><span style=color:#a6e22e>fromTable</span><span style=color:#f92672>(</span>table<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>IcebergSource<span style=color:#f92672>&lt;</span>GenericRecord<span style=color:#f92672>&gt;</span> source <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    IcebergSource<span style=color:#f92672>.&lt;</span>GenericRecord<span style=color:#f92672>&gt;</span>builder<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span><span style=color:#a6e22e>tableLoader</span><span style=color:#f92672>(</span>tableLoader<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span><span style=color:#a6e22e>readerFunction</span><span style=color:#f92672>(</span>readerFunction<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span><span style=color:#a6e22e>assignerFactory</span><span style=color:#f92672>(</span><span style=color:#66d9ef>new</span> SimpleSplitAssignerFactory<span style=color:#f92672>())</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span><span style=color:#a6e22e>build</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>DataStream<span style=color:#f92672>&lt;</span>Row<span style=color:#f92672>&gt;</span> stream <span style=color:#f92672>=</span> env<span style=color:#f92672>.</span><span style=color:#a6e22e>fromSource</span><span style=color:#f92672>(</span>source<span style=color:#f92672>,</span> WatermarkStrategy<span style=color:#f92672>.</span><span style=color:#a6e22e>noWatermarks</span><span style=color:#f92672>(),</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Iceberg Source as Avro GenericRecord&#34;</span><span style=color:#f92672>,</span> <span style=color:#66d9ef>new</span> GenericRecordAvroTypeInfo<span style=color:#f92672>(</span>avroSchema<span style=color:#f92672>));</span>
</span></span></code></pre></div><h2 id=writing-with-datastream>Writing with DataStream</h2><p>Iceberg support writing to iceberg table from different DataStream input.</p><h3 id=appending-data>Appending data.</h3><p>Flink supports writing <code>DataStream&lt;RowData></code> and <code>DataStream&lt;Row></code> to the sink iceberg table natively.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span>StreamExecutionEnvironment env <span style=color:#f92672>=</span> <span style=color:#f92672>...;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>DataStream<span style=color:#f92672>&lt;</span>RowData<span style=color:#f92672>&gt;</span> input <span style=color:#f92672>=</span> <span style=color:#f92672>...</span> <span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>Configuration hadoopConf <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> Configuration<span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>TableLoader tableLoader <span style=color:#f92672>=</span> TableLoader<span style=color:#f92672>.</span><span style=color:#a6e22e>fromHadoopTable</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;hdfs://nn:8020/warehouse/path&#34;</span><span style=color:#f92672>,</span> hadoopConf<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>FlinkSink<span style=color:#f92672>.</span><span style=color:#a6e22e>forRowData</span><span style=color:#f92672>(</span>input<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>tableLoader</span><span style=color:#f92672>(</span>tableLoader<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>append</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>env<span style=color:#f92672>.</span><span style=color:#a6e22e>execute</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Test Iceberg DataStream&#34;</span><span style=color:#f92672>);</span>
</span></span></code></pre></div><p>The iceberg API also allows users to write generic <code>DataStream&lt;T></code> to iceberg table, more example could be found in this <a href=https://github.com/apache/iceberg/blob/master/flink/v1.16/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java>unit test</a>.</p><h3 id=overwrite-data>Overwrite data</h3><p>Set the <code>overwrite</code> flag in FlinkSink builder to overwrite the data in existing iceberg tables:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span>StreamExecutionEnvironment env <span style=color:#f92672>=</span> <span style=color:#f92672>...;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>DataStream<span style=color:#f92672>&lt;</span>RowData<span style=color:#f92672>&gt;</span> input <span style=color:#f92672>=</span> <span style=color:#f92672>...</span> <span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>Configuration hadoopConf <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> Configuration<span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>TableLoader tableLoader <span style=color:#f92672>=</span> TableLoader<span style=color:#f92672>.</span><span style=color:#a6e22e>fromHadoopTable</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;hdfs://nn:8020/warehouse/path&#34;</span><span style=color:#f92672>,</span> hadoopConf<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>FlinkSink<span style=color:#f92672>.</span><span style=color:#a6e22e>forRowData</span><span style=color:#f92672>(</span>input<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>tableLoader</span><span style=color:#f92672>(</span>tableLoader<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>overwrite</span><span style=color:#f92672>(</span><span style=color:#66d9ef>true</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>append</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>env<span style=color:#f92672>.</span><span style=color:#a6e22e>execute</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Test Iceberg DataStream&#34;</span><span style=color:#f92672>);</span>
</span></span></code></pre></div><h3 id=upsert-data>Upsert data</h3><p>Set the <code>upsert</code> flag in FlinkSink builder to upsert the data in existing iceberg table. The table must use v2 table format and have a primary key.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span>StreamExecutionEnvironment env <span style=color:#f92672>=</span> <span style=color:#f92672>...;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>DataStream<span style=color:#f92672>&lt;</span>RowData<span style=color:#f92672>&gt;</span> input <span style=color:#f92672>=</span> <span style=color:#f92672>...</span> <span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>Configuration hadoopConf <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> Configuration<span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>TableLoader tableLoader <span style=color:#f92672>=</span> TableLoader<span style=color:#f92672>.</span><span style=color:#a6e22e>fromHadoopTable</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;hdfs://nn:8020/warehouse/path&#34;</span><span style=color:#f92672>,</span> hadoopConf<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>FlinkSink<span style=color:#f92672>.</span><span style=color:#a6e22e>forRowData</span><span style=color:#f92672>(</span>input<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>tableLoader</span><span style=color:#f92672>(</span>tableLoader<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>upsert</span><span style=color:#f92672>(</span><span style=color:#66d9ef>true</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span><span style=color:#a6e22e>append</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>env<span style=color:#f92672>.</span><span style=color:#a6e22e>execute</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Test Iceberg DataStream&#34;</span><span style=color:#f92672>);</span>
</span></span></code></pre></div><div class=info>OVERWRITE and UPSERT can&rsquo;t be set together. In UPSERT mode, if the table is partitioned, the partition fields should be included in equality fields.</div><h3 id=write-with-avro-genericrecord>Write with Avro GenericRecord</h3><p>Flink Iceberg sink provides <code>AvroGenericRecordToRowDataMapper</code> that converts
Avro <code>GenericRecord</code> to Flink <code>RowData</code>. You can use the mapper to write
Avro GenericRecord DataStream to Iceberg.</p><p>Please make sure <code>flink-avro</code> jar is included in the classpath.
Also <code>iceberg-flink-runtime</code> shaded bundle jar can&rsquo;t be used
because the runtime jar shades the avro package.
Please use non-shaded <code>iceberg-flink</code> jar instead.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span>DataStream<span style=color:#f92672>&lt;</span>org<span style=color:#f92672>.</span><span style=color:#a6e22e>apache</span><span style=color:#f92672>.</span><span style=color:#a6e22e>avro</span><span style=color:#f92672>.</span><span style=color:#a6e22e>generic</span><span style=color:#f92672>.</span><span style=color:#a6e22e>GenericRecord</span><span style=color:#f92672>&gt;</span> dataStream <span style=color:#f92672>=</span> <span style=color:#f92672>...;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Schema icebergSchema <span style=color:#f92672>=</span> table<span style=color:#f92672>.</span><span style=color:#a6e22e>schema</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// The Avro schema converted from Iceberg schema can&#39;t be used
</span></span></span><span style=display:flex><span><span style=color:#75715e>// due to precision difference between how Iceberg schema (micro)
</span></span></span><span style=display:flex><span><span style=color:#75715e>// and Flink AvroToRowDataConverters (milli) deal with time type.
</span></span></span><span style=display:flex><span><span style=color:#75715e>// Instead, use the Avro schema defined directly.
</span></span></span><span style=display:flex><span><span style=color:#75715e>// See AvroGenericRecordToRowDataMapper Javadoc for more details.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>org<span style=color:#f92672>.</span><span style=color:#a6e22e>apache</span><span style=color:#f92672>.</span><span style=color:#a6e22e>avro</span><span style=color:#f92672>.</span><span style=color:#a6e22e>Schema</span> avroSchema <span style=color:#f92672>=</span> AvroSchemaUtil<span style=color:#f92672>.</span><span style=color:#a6e22e>convert</span><span style=color:#f92672>(</span>icebergSchema<span style=color:#f92672>,</span> table<span style=color:#f92672>.</span><span style=color:#a6e22e>name</span><span style=color:#f92672>());</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>GenericRecordAvroTypeInfo avroTypeInfo <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> GenericRecordAvroTypeInfo<span style=color:#f92672>(</span>avroSchema<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>RowType rowType <span style=color:#f92672>=</span> FlinkSchemaUtil<span style=color:#f92672>.</span><span style=color:#a6e22e>convert</span><span style=color:#f92672>(</span>icebergSchema<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>FlinkSink<span style=color:#f92672>.</span><span style=color:#a6e22e>builderFor</span><span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>    dataStream<span style=color:#f92672>,</span>
</span></span><span style=display:flex><span>    AvroGenericRecordToRowDataMapper<span style=color:#f92672>.</span><span style=color:#a6e22e>forAvroSchema</span><span style=color:#f92672>(</span>avroSchema<span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>    FlinkCompatibilityUtil<span style=color:#f92672>.</span><span style=color:#a6e22e>toTypeInfo</span><span style=color:#f92672>(</span>rowType<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span><span style=color:#a6e22e>table</span><span style=color:#f92672>(</span>table<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span><span style=color:#a6e22e>tableLoader</span><span style=color:#f92672>(</span>tableLoader<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span><span style=color:#a6e22e>append</span><span style=color:#f92672>();</span>
</span></span></code></pre></div><h3 id=netrics>Netrics</h3><p>The following Flink metrics are provided by the Flink Iceberg sink.</p><p>Parallel writer metrics are added under the sub group of <code>IcebergStreamWriter</code>.
They should have the following key-value tags.</p><ul><li>table: full table name (like iceberg.my_db.my_table)</li><li>subtask_index: writer subtask index starting from 0</li></ul><table><thead><tr><th>Metric name</th><th>Metric type</th><th>Description</th></tr></thead><tbody><tr><td>lastFlushDurationMs</td><td>Gague</td><td>The duration (in milli) that writer subtasks take to flush and upload the files during checkpoint.</td></tr><tr><td>flushedDataFiles</td><td>Counter</td><td>Number of data files flushed and uploaded.</td></tr><tr><td>flushedDeleteFiles</td><td>Counter</td><td>Number of delete files flushed and uploaded.</td></tr><tr><td>flushedReferencedDataFiles</td><td>Counter</td><td>Number of data files referenced by the flushed delete files.</td></tr><tr><td>dataFilesSizeHistogram</td><td>Histogram</td><td>Histogram distribution of data file sizes (in bytes).</td></tr><tr><td>deleteFilesSizeHistogram</td><td>Histogram</td><td>Histogram distribution of delete file sizes (in bytes).</td></tr></tbody></table><p>Committer metrics are added under the sub group of <code>IcebergFilesCommitter</code>.
They should have the following key-value tags.</p><ul><li>table: full table name (like iceberg.my_db.my_table)</li></ul><table><thead><tr><th>Metric name</th><th>Metric type</th><th>Description</th></tr></thead><tbody><tr><td>lastCheckpointDurationMs</td><td>Gague</td><td>The duration (in milli) that the committer operator checkpoints its state.</td></tr><tr><td>lastCommitDurationMs</td><td>Gague</td><td>The duration (in milli) that the Iceberg table commit takes.</td></tr><tr><td>committedDataFilesCount</td><td>Counter</td><td>Number of data files committed.</td></tr><tr><td>committedDataFilesRecordCount</td><td>Counter</td><td>Number of records contained in the committed data files.</td></tr><tr><td>committedDataFilesByteCount</td><td>Counter</td><td>Number of bytes contained in the committed data files.</td></tr><tr><td>committedDeleteFilesCount</td><td>Counter</td><td>Number of delete files committed.</td></tr><tr><td>committedDeleteFilesRecordCount</td><td>Counter</td><td>Number of records contained in the committed delete files.</td></tr><tr><td>committedDeleteFilesByteCount</td><td>Counter</td><td>Number of bytes contained in the committed delete files.</td></tr><tr><td>elapsedSecondsSinceLastSuccessfulCommit</td><td>Gague</td><td>Elapsed time (in seconds) since last successful Iceberg commit.</td></tr></tbody></table><p><code>elapsedSecondsSinceLastSuccessfulCommit</code> is an ideal alerting metric
to detect failed or missing Iceberg commits.</p><ul><li>Iceberg commit happened after successful Flink checkpoint in the <code>notifyCheckpointComplete</code> callback.
It could happen that Iceberg commits failed (for whatever reason), while Flink checkpoints succeeding.</li><li>It could also happen that <code>notifyCheckpointComplete</code> wasn&rsquo;t triggered (for whatever bug).
As a result, there won&rsquo;t be any Iceberg commits attempted.</li></ul><p>If the checkpoint interval (and expected Iceberg commit interval) is 5 minutes, set up alert with rule like <code>elapsedSecondsSinceLastSuccessfulCommit > 60 minutes</code> to detect failed or missing Iceberg commits in the past hour.</p><h2 id=options>Options</h2><h3 id=read-options>Read options</h3><p>Flink read options are passed when configuring the Flink IcebergSource:</p><pre tabindex=0><code>IcebergSource.forRowData()
    .tableLoader(TableLoader.fromCatalog(...))
    .assignerFactory(new SimpleSplitAssignerFactory())
    .streaming(true)
    .streamingStartingStrategy(StreamingStartingStrategy.INCREMENTAL_FROM_LATEST_SNAPSHOT)
    .startSnapshotId(3821550127947089987L)
    .monitorInterval(Duration.ofMillis(10L)) // or .set(&#34;monitor-interval&#34;, &#34;10s&#34;) \ set(FlinkReadOptions.MONITOR_INTERVAL, &#34;10s&#34;)
    .build()
</code></pre><p>For Flink SQL, read options can be passed in via SQL hints like this:</p><pre tabindex=0><code>SELECT * FROM tableName /*+ OPTIONS(&#39;monitor-interval&#39;=&#39;10s&#39;) */
...
</code></pre><p>Options can be passed in via Flink configuration, which will be applied to current session. Note that not all options support this mode.</p><pre tabindex=0><code>env.getConfig()
    .getConfiguration()
    .set(FlinkReadOptions.SPLIT_FILE_OPEN_COST_OPTION, 1000L);
...
</code></pre><p><code>Read option</code> has the highest priority, followed by <code>Flink configuration</code> and then <code>Table property</code>.</p><table><thead><tr><th>Read option</th><th>Flink configuration</th><th>Table property</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td>snapshot-id</td><td>N/A</td><td>N/A</td><td>null</td><td>For time travel in batch mode. Read data from the specified snapshot-id.</td></tr><tr><td>case-sensitive</td><td>connector.iceberg.case-sensitive</td><td>N/A</td><td>false</td><td>If true, match column name in a case sensitive way.</td></tr><tr><td>as-of-timestamp</td><td>N/A</td><td>N/A</td><td>null</td><td>For time travel in batch mode. Read data from the most recent snapshot as of the given time in milliseconds.</td></tr><tr><td>starting-strategy</td><td>connector.iceberg.starting-strategy</td><td>N/A</td><td>INCREMENTAL_FROM_LATEST_SNAPSHOT</td><td>Starting strategy for streaming execution. TABLE_SCAN_THEN_INCREMENTAL: Do a regular table scan then switch to the incremental mode. The incremental mode starts from the current snapshot exclusive. INCREMENTAL_FROM_LATEST_SNAPSHOT: Start incremental mode from the latest snapshot inclusive. If it is an empty map, all future append snapshots should be discovered. INCREMENTAL_FROM_EARLIEST_SNAPSHOT: Start incremental mode from the earliest snapshot inclusive. If it is an empty map, all future append snapshots should be discovered. INCREMENTAL_FROM_SNAPSHOT_ID: Start incremental mode from a snapshot with a specific id inclusive. INCREMENTAL_FROM_SNAPSHOT_TIMESTAMP: Start incremental mode from a snapshot with a specific timestamp inclusive. If the timestamp is between two snapshots, it should start from the snapshot after the timestamp. Just for FIP27 Source.</td></tr><tr><td>start-snapshot-timestamp</td><td>N/A</td><td>N/A</td><td>null</td><td>Start to read data from the most recent snapshot as of the given time in milliseconds.</td></tr><tr><td>start-snapshot-id</td><td>N/A</td><td>N/A</td><td>null</td><td>Start to read data from the specified snapshot-id.</td></tr><tr><td>end-snapshot-id</td><td>N/A</td><td>N/A</td><td>The latest snapshot id</td><td>Specifies the end snapshot.</td></tr><tr><td>split-size</td><td>connector.iceberg.split-size</td><td>read.split.target-size</td><td>128 MB</td><td>Target size when combining input splits.</td></tr><tr><td>split-lookback</td><td>connector.iceberg.split-file-open-cost</td><td>read.split.planning-lookback</td><td>10</td><td>Number of bins to consider when combining input splits.</td></tr><tr><td>split-file-open-cost</td><td>connector.iceberg.split-file-open-cost</td><td>read.split.open-file-cost</td><td>4MB</td><td>The estimated cost to open a file, used as a minimum weight when combining splits.</td></tr><tr><td>streaming</td><td>connector.iceberg.streaming</td><td>N/A</td><td>false</td><td>Sets whether the current task runs in streaming or batch mode.</td></tr><tr><td>monitor-interval</td><td>connector.iceberg.monitor-interval</td><td>N/A</td><td>60s</td><td>Monitor interval to discover splits from new snapshots. Applicable only for streaming read.</td></tr><tr><td>include-column-stats</td><td>connector.iceberg.include-column-stats</td><td>N/A</td><td>false</td><td>Create a new scan from this that loads the column stats with each data file. Column stats include: value count, null value count, lower bounds, and upper bounds.</td></tr><tr><td>max-planning-snapshot-count</td><td>connector.iceberg.max-planning-snapshot-count</td><td>N/A</td><td>Integer.MAX_VALUE</td><td>Max number of snapshots limited per split enumeration. Applicable only to streaming read.</td></tr><tr><td>limit</td><td>connector.iceberg.limit</td><td>N/A</td><td>-1</td><td>Limited output number of rows.</td></tr></tbody></table><h3 id=write-options>Write options</h3><p>Flink write options are passed when configuring the FlinkSink, like this:</p><pre tabindex=0><code>FlinkSink.Builder builder = FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)
    .table(table)
    .tableLoader(tableLoader)
    .set(&#34;write-format&#34;, &#34;orc&#34;)
    .set(FlinkWriteOptions.OVERWRITE_MODE, &#34;true&#34;);
</code></pre><p>For Flink SQL, write options can be passed in via SQL hints like this:</p><pre tabindex=0><code>INSERT INTO tableName /*+ OPTIONS(&#39;upsert-enabled&#39;=&#39;true&#39;) */
...
</code></pre><table><thead><tr><th>Flink option</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td>write-format</td><td>Table write.format.default</td><td>File format to use for this write operation; parquet, avro, or orc</td></tr><tr><td>target-file-size-bytes</td><td>As per table property</td><td>Overrides this table&rsquo;s write.target-file-size-bytes</td></tr><tr><td>upsert-enabled</td><td>Table write.upsert.enabled</td><td>Overrides this table&rsquo;s write.upsert.enabled</td></tr><tr><td>overwrite-enabled</td><td>false</td><td>Overwrite the table&rsquo;s data, overwrite mode shouldn&rsquo;t be enable when configuring to use UPSERT data stream.</td></tr><tr><td>distribution-mode</td><td>Table write.distribution-mode</td><td>Overrides this table&rsquo;s write.distribution-mode</td></tr><tr><td>compression-codec</td><td>Table write.(fileformat).compression-codec</td><td>Overrides this table&rsquo;s compression codec for this write</td></tr><tr><td>compression-level</td><td>Table write.(fileformat).compression-level</td><td>Overrides this table&rsquo;s compression level for Parquet and Avro tables for this write</td></tr><tr><td>compression-strategy</td><td>Table write.orc.compression-strategy</td><td>Overrides this table&rsquo;s compression strategy for ORC tables for this write</td></tr></tbody></table><h2 id=inspecting-tables>Inspecting tables</h2><p>To inspect a table&rsquo;s history, snapshots, and other metadata, Iceberg supports metadata tables.</p><p>Metadata tables are identified by adding the metadata table name after the original table name. For example, history for <code>db.table</code> is read using <code>db.table$history</code>.</p><h3 id=history>History</h3><p>To show table history:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> prod.db.<span style=color:#66d9ef>table</span><span style=color:#960050;background-color:#1e0010>$</span>history;
</span></span></code></pre></div><table><thead><tr><th>made_current_at</th><th>snapshot_id</th><th>parent_id</th><th>is_current_ancestor</th></tr></thead><tbody><tr><td>2019-02-08 03:29:51.215</td><td>5781947118336215154</td><td>NULL</td><td>true</td></tr><tr><td>2019-02-08 03:47:55.948</td><td>5179299526185056830</td><td>5781947118336215154</td><td>true</td></tr><tr><td>2019-02-09 16:24:30.13</td><td>296410040247533544</td><td>5179299526185056830</td><td>false</td></tr><tr><td>2019-02-09 16:32:47.336</td><td>2999875608062437330</td><td>5179299526185056830</td><td>true</td></tr><tr><td>2019-02-09 19:42:03.919</td><td>8924558786060583479</td><td>2999875608062437330</td><td>true</td></tr><tr><td>2019-02-09 19:49:16.343</td><td>6536733823181975045</td><td>8924558786060583479</td><td>true</td></tr></tbody></table><div class=info><strong>This shows a commit that was rolled back.</strong> In this example, snapshot 296410040247533544 and 2999875608062437330 have the same parent snapshot 5179299526185056830. Snapshot 296410040247533544 was rolled back and is <em>not</em> an ancestor of the current table state.</div><h3 id=metadata-log-entries>Metadata Log Entries</h3><p>To show table metadata log entries:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>from</span> prod.db.<span style=color:#66d9ef>table</span><span style=color:#960050;background-color:#1e0010>$</span>metadata_log_entries;
</span></span></code></pre></div><table><thead><tr><th>timestamp</th><th>file</th><th>latest_snapshot_id</th><th>latest_schema_id</th><th>latest_sequence_number</th></tr></thead><tbody><tr><td>2022-07-28 10:43:52.93</td><td>s3://&mldr;/table/metadata/00000-9441e604-b3c2-498a-a45a-6320e8ab9006.metadata.json</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2022-07-28 10:43:57.487</td><td>s3://&mldr;/table/metadata/00001-f30823df-b745-4a0a-b293-7532e0c99986.metadata.json</td><td>170260833677645300</td><td>0</td><td>1</td></tr><tr><td>2022-07-28 10:43:58.25</td><td>s3://&mldr;/table/metadata/00002-2cc2837a-02dc-4687-acc1-b4d86ea486f4.metadata.json</td><td>958906493976709774</td><td>0</td><td>2</td></tr></tbody></table><h3 id=snapshots>Snapshots</h3><p>To show the valid snapshots for a table:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> prod.db.<span style=color:#66d9ef>table</span><span style=color:#960050;background-color:#1e0010>$</span>snapshots;
</span></span></code></pre></div><table><thead><tr><th>committed_at</th><th>snapshot_id</th><th>parent_id</th><th>operation</th><th>manifest_list</th><th>summary</th></tr></thead><tbody><tr><td>2019-02-08 03:29:51.215</td><td>57897183625154</td><td>null</td><td>append</td><td>s3://&mldr;/table/metadata/snap-57897183625154-1.avro</td><td>{ added-records -> 2478404, total-records -> 2478404, added-data-files -> 438, total-data-files -> 438, flink.job-id -> 2e274eecb503d85369fb390e8956c813 }</td></tr></tbody></table><p>You can also join snapshots to table history. For example, this query will show table history, with the application ID that wrote each snapshot:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>select</span>
</span></span><span style=display:flex><span>    h.made_current_at,
</span></span><span style=display:flex><span>    s.<span style=color:#66d9ef>operation</span>,
</span></span><span style=display:flex><span>    h.snapshot_id,
</span></span><span style=display:flex><span>    h.is_current_ancestor,
</span></span><span style=display:flex><span>    s.summary[<span style=color:#e6db74>&#39;flink.job-id&#39;</span>]
</span></span><span style=display:flex><span><span style=color:#66d9ef>from</span> prod.db.<span style=color:#66d9ef>table</span><span style=color:#960050;background-color:#1e0010>$</span>history h
</span></span><span style=display:flex><span><span style=color:#66d9ef>join</span> prod.db.<span style=color:#66d9ef>table</span><span style=color:#960050;background-color:#1e0010>$</span>snapshots s
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>on</span> h.snapshot_id <span style=color:#f92672>=</span> s.snapshot_id
</span></span><span style=display:flex><span><span style=color:#66d9ef>order</span> <span style=color:#66d9ef>by</span> made_current_at
</span></span></code></pre></div><table><thead><tr><th>made_current_at</th><th>operation</th><th>snapshot_id</th><th>is_current_ancestor</th><th>summary[flink.job-id]</th></tr></thead><tbody><tr><td>2019-02-08 03:29:51.215</td><td>append</td><td>57897183625154</td><td>true</td><td>2e274eecb503d85369fb390e8956c813</td></tr></tbody></table><h3 id=files>Files</h3><p>To show a table&rsquo;s current data files:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> prod.db.<span style=color:#66d9ef>table</span><span style=color:#960050;background-color:#1e0010>$</span>files;
</span></span></code></pre></div><table><thead><tr><th>content</th><th>file_path</th><th>file_format</th><th>spec_id</th><th>partition</th><th>record_count</th><th>file_size_in_bytes</th><th>column_sizes</th><th>value_counts</th><th>null_value_counts</th><th>nan_value_counts</th><th>lower_bounds</th><th>upper_bounds</th><th>key_metadata</th><th>split_offsets</th><th>equality_ids</th><th>sort_order_id</th></tr></thead><tbody><tr><td>0</td><td>s3:/&mldr;/table/data/00000-3-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet</td><td>PARQUET</td><td>0</td><td>{1999-01-01, 01}</td><td>1</td><td>597</td><td>[1 -> 90, 2 -> 62]</td><td>[1 -> 1, 2 -> 1]</td><td>[1 -> 0, 2 -> 0]</td><td>[]</td><td>[1 -> , 2 -> c]</td><td>[1 -> , 2 -> c]</td><td>null</td><td>[4]</td><td>null</td><td>null</td></tr><tr><td>0</td><td>s3:/&mldr;/table/data/00001-4-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet</td><td>PARQUET</td><td>0</td><td>{1999-01-01, 02}</td><td>1</td><td>597</td><td>[1 -> 90, 2 -> 62]</td><td>[1 -> 1, 2 -> 1]</td><td>[1 -> 0, 2 -> 0]</td><td>[]</td><td>[1 -> , 2 -> b]</td><td>[1 -> , 2 -> b]</td><td>null</td><td>[4]</td><td>null</td><td>null</td></tr><tr><td>0</td><td>s3:/&mldr;/table/data/00002-5-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet</td><td>PARQUET</td><td>0</td><td>{1999-01-01, 03}</td><td>1</td><td>597</td><td>[1 -> 90, 2 -> 62]</td><td>[1 -> 1, 2 -> 1]</td><td>[1 -> 0, 2 -> 0]</td><td>[]</td><td>[1 -> , 2 -> a]</td><td>[1 -> , 2 -> a]</td><td>null</td><td>[4]</td><td>null</td><td>null</td></tr></tbody></table><h3 id=manifests>Manifests</h3><p>To show a table&rsquo;s current file manifests:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> prod.db.<span style=color:#66d9ef>table</span><span style=color:#960050;background-color:#1e0010>$</span>manifests;
</span></span></code></pre></div><table><thead><tr><th>path</th><th>length</th><th>partition_spec_id</th><th>added_snapshot_id</th><th>added_data_files_count</th><th>existing_data_files_count</th><th>deleted_data_files_count</th><th>partition_summaries</th></tr></thead><tbody><tr><td>s3://&mldr;/table/metadata/45b5290b-ee61-4788-b324-b1e2735c0e10-m0.avro</td><td>4479</td><td>0</td><td>6668963634911763636</td><td>8</td><td>0</td><td>0</td><td>[[false,null,2019-05-13,2019-05-15]]</td></tr></tbody></table><p>Note:</p><ol><li>Fields within <code>partition_summaries</code> column of the manifests table correspond to <code>field_summary</code> structs within <a href=../../../spec#manifest-lists>manifest list</a>, with the following order:<ul><li><code>contains_null</code></li><li><code>contains_nan</code></li><li><code>lower_bound</code></li><li><code>upper_bound</code></li></ul></li><li><code>contains_nan</code> could return null, which indicates that this information is not available from the file&rsquo;s metadata.
This usually occurs when reading from V1 table, where <code>contains_nan</code> is not populated.</li></ol><h3 id=partitions>Partitions</h3><p>To show a table&rsquo;s current partitions:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> prod.db.<span style=color:#66d9ef>table</span><span style=color:#960050;background-color:#1e0010>$</span>partitions;
</span></span></code></pre></div><table><thead><tr><th>partition</th><th>record_count</th><th>file_count</th><th>spec_id</th></tr></thead><tbody><tr><td>{20211001, 11}</td><td>1</td><td>1</td><td>0</td></tr><tr><td>{20211002, 11}</td><td>1</td><td>1</td><td>0</td></tr><tr><td>{20211001, 10}</td><td>1</td><td>1</td><td>0</td></tr><tr><td>{20211002, 10}</td><td>1</td><td>1</td><td>0</td></tr></tbody></table><p>Note:
For unpartitioned tables, the partitions table will contain only the record_count and file_count columns.</p><h3 id=all-metadata-tables>All Metadata Tables</h3><p>These tables are unions of the metadata tables specific to the current snapshot, and return metadata across all snapshots.</p><div class=danger>The &ldquo;all&rdquo; metadata tables may produce more than one row per data file or manifest file because metadata files may be part of more than one table snapshot.</div><h4 id=all-data-files>All Data Files</h4><p>To show all of the table&rsquo;s data files and each file&rsquo;s metadata:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> prod.db.<span style=color:#66d9ef>table</span><span style=color:#960050;background-color:#1e0010>$</span>all_data_files;
</span></span></code></pre></div><table><thead><tr><th>content</th><th>file_path</th><th>file_format</th><th>partition</th><th>record_count</th><th>file_size_in_bytes</th><th>column_sizes</th><th>value_counts</th><th>null_value_counts</th><th>nan_value_counts</th><th>lower_bounds</th><th>upper_bounds</th><th>key_metadata</th><th>split_offsets</th><th>equality_ids</th><th>sort_order_id</th></tr></thead><tbody><tr><td>0</td><td>s3://&mldr;/dt=20210102/00000-0-756e2512-49ae-45bb-aae3-c0ca475e7879-00001.parquet</td><td>PARQUET</td><td>{20210102}</td><td>14</td><td>2444</td><td>{1 -> 94, 2 -> 17}</td><td>{1 -> 14, 2 -> 14}</td><td>{1 -> 0, 2 -> 0}</td><td>{}</td><td>{1 -> 1, 2 -> 20210102}</td><td>{1 -> 2, 2 -> 20210102}</td><td>null</td><td>[4]</td><td>null</td><td>0</td></tr><tr><td>0</td><td>s3://&mldr;/dt=20210103/00000-0-26222098-032f-472b-8ea5-651a55b21210-00001.parquet</td><td>PARQUET</td><td>{20210103}</td><td>14</td><td>2444</td><td>{1 -> 94, 2 -> 17}</td><td>{1 -> 14, 2 -> 14}</td><td>{1 -> 0, 2 -> 0}</td><td>{}</td><td>{1 -> 1, 2 -> 20210103}</td><td>{1 -> 3, 2 -> 20210103}</td><td>null</td><td>[4]</td><td>null</td><td>0</td></tr><tr><td>0</td><td>s3://&mldr;/dt=20210104/00000-0-a3bb1927-88eb-4f1c-bc6e-19076b0d952e-00001.parquet</td><td>PARQUET</td><td>{20210104}</td><td>14</td><td>2444</td><td>{1 -> 94, 2 -> 17}</td><td>{1 -> 14, 2 -> 14}</td><td>{1 -> 0, 2 -> 0}</td><td>{}</td><td>{1 -> 1, 2 -> 20210104}</td><td>{1 -> 3, 2 -> 20210104}</td><td>null</td><td>[4]</td><td>null</td><td>0</td></tr></tbody></table><h4 id=all-manifests>All Manifests</h4><p>To show all of the table&rsquo;s manifest files:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> prod.db.<span style=color:#66d9ef>table</span><span style=color:#960050;background-color:#1e0010>$</span>all_manifests;
</span></span></code></pre></div><table><thead><tr><th>path</th><th>length</th><th>partition_spec_id</th><th>added_snapshot_id</th><th>added_data_files_count</th><th>existing_data_files_count</th><th>deleted_data_files_count</th><th>partition_summaries</th></tr></thead><tbody><tr><td>s3://&mldr;/metadata/a85f78c5-3222-4b37-b7e4-faf944425d48-m0.avro</td><td>6376</td><td>0</td><td>6272782676904868561</td><td>2</td><td>0</td><td>0</td><td>[{false, false, 20210101, 20210101}]</td></tr></tbody></table><p>Note:</p><ol><li>Fields within <code>partition_summaries</code> column of the manifests table correspond to <code>field_summary</code> structs within <a href=../../../spec#manifest-lists>manifest list</a>, with the following order:<ul><li><code>contains_null</code></li><li><code>contains_nan</code></li><li><code>lower_bound</code></li><li><code>upper_bound</code></li></ul></li><li><code>contains_nan</code> could return null, which indicates that this information is not available from the file&rsquo;s metadata.
This usually occurs when reading from V1 table, where <code>contains_nan</code> is not populated.</li></ol><h3 id=references>References</h3><p>To show a table&rsquo;s known snapshot references:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>FROM</span> prod.db.<span style=color:#66d9ef>table</span><span style=color:#960050;background-color:#1e0010>$</span>refs;
</span></span></code></pre></div><table><thead><tr><th>name</th><th>type</th><th>snapshot_id</th><th>max_reference_age_in_ms</th><th>min_snapshots_to_keep</th><th>max_snapshot_age_in_ms</th></tr></thead><tbody><tr><td>main</td><td>BRANCH</td><td>4686954189838128572</td><td>10</td><td>20</td><td>30</td></tr><tr><td>testTag</td><td>TAG</td><td>4686954189838128572</td><td>10</td><td>null</td><td>null</td></tr></tbody></table><h2 id=rewrite-files-action>Rewrite files action.</h2><p>Iceberg provides API to rewrite small files into large files by submitting flink batch job. The behavior of this flink action is the same as the spark&rsquo;s <a href=../maintenance/#compact-data-files>rewriteDataFiles</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span><span style=color:#f92672>import</span> org.apache.iceberg.flink.actions.Actions<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>TableLoader tableLoader <span style=color:#f92672>=</span> TableLoader<span style=color:#f92672>.</span><span style=color:#a6e22e>fromHadoopTable</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;hdfs://nn:8020/warehouse/path&#34;</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>Table table <span style=color:#f92672>=</span> tableLoader<span style=color:#f92672>.</span><span style=color:#a6e22e>loadTable</span><span style=color:#f92672>();</span>
</span></span><span style=display:flex><span>RewriteDataFilesActionResult result <span style=color:#f92672>=</span> Actions<span style=color:#f92672>.</span><span style=color:#a6e22e>forTable</span><span style=color:#f92672>(</span>table<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span><span style=color:#a6e22e>rewriteDataFiles</span><span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span><span style=color:#a6e22e>execute</span><span style=color:#f92672>();</span>
</span></span></code></pre></div><p>For more doc about options of the rewrite files action, please see <a href=../../../javadoc/1.2.0/org/apache/iceberg/flink/actions/RewriteDataFilesAction.html>RewriteDataFilesAction</a></p><h2 id=type-conversion>Type conversion</h2><p>Iceberg&rsquo;s integration for Flink automatically converts between Flink and Iceberg types. When writing to a table with types that are not supported by Flink, like UUID, Iceberg will accept and convert values from the Flink type.</p><h3 id=flink-to-iceberg>Flink to Iceberg</h3><p>Flink types are converted to Iceberg types according to the following table:</p><table><thead><tr><th>Flink</th><th>Iceberg</th><th>Notes</th></tr></thead><tbody><tr><td>boolean</td><td>boolean</td><td></td></tr><tr><td>tinyint</td><td>integer</td><td></td></tr><tr><td>smallint</td><td>integer</td><td></td></tr><tr><td>integer</td><td>integer</td><td></td></tr><tr><td>bigint</td><td>long</td><td></td></tr><tr><td>float</td><td>float</td><td></td></tr><tr><td>double</td><td>double</td><td></td></tr><tr><td>char</td><td>string</td><td></td></tr><tr><td>varchar</td><td>string</td><td></td></tr><tr><td>string</td><td>string</td><td></td></tr><tr><td>binary</td><td>binary</td><td></td></tr><tr><td>varbinary</td><td>fixed</td><td></td></tr><tr><td>decimal</td><td>decimal</td><td></td></tr><tr><td>date</td><td>date</td><td></td></tr><tr><td>time</td><td>time</td><td></td></tr><tr><td>timestamp</td><td>timestamp without timezone</td><td></td></tr><tr><td>timestamp_ltz</td><td>timestamp with timezone</td><td></td></tr><tr><td>array</td><td>list</td><td></td></tr><tr><td>map</td><td>map</td><td></td></tr><tr><td>multiset</td><td>map</td><td></td></tr><tr><td>row</td><td>struct</td><td></td></tr><tr><td>raw</td><td></td><td>Not supported</td></tr><tr><td>interval</td><td></td><td>Not supported</td></tr><tr><td>structured</td><td></td><td>Not supported</td></tr><tr><td>timestamp with zone</td><td></td><td>Not supported</td></tr><tr><td>distinct</td><td></td><td>Not supported</td></tr><tr><td>null</td><td></td><td>Not supported</td></tr><tr><td>symbol</td><td></td><td>Not supported</td></tr><tr><td>logical</td><td></td><td>Not supported</td></tr></tbody></table><h3 id=iceberg-to-flink>Iceberg to Flink</h3><p>Iceberg types are converted to Flink types according to the following table:</p><table><thead><tr><th>Iceberg</th><th>Flink</th></tr></thead><tbody><tr><td>boolean</td><td>boolean</td></tr><tr><td>struct</td><td>row</td></tr><tr><td>list</td><td>array</td></tr><tr><td>map</td><td>map</td></tr><tr><td>integer</td><td>integer</td></tr><tr><td>long</td><td>bigint</td></tr><tr><td>float</td><td>float</td></tr><tr><td>double</td><td>double</td></tr><tr><td>date</td><td>date</td></tr><tr><td>time</td><td>time</td></tr><tr><td>timestamp without timezone</td><td>timestamp(6)</td></tr><tr><td>timestamp with timezone</td><td>timestamp_ltz(6)</td></tr><tr><td>string</td><td>varchar(2147483647)</td></tr><tr><td>uuid</td><td>binary(16)</td></tr><tr><td>fixed(N)</td><td>binary(N)</td></tr><tr><td>binary</td><td>varbinary(2147483647)</td></tr><tr><td>decimal(P, S)</td><td>decimal(P, S)</td></tr></tbody></table><h2 id=future-improvement>Future improvement.</h2><p>There are some features that are do not yet supported in the current Flink Iceberg integration work:</p><ul><li>Don&rsquo;t support creating iceberg table with hidden partitioning. <a href=http://mail-archives.apache.org/mod_mbox/flink-dev/202008.mbox/%3cCABi+2jQCo3MsOa4+ywaxV5J-Z8TGKNZDX-pQLYB-dG+dVUMiMw@mail.gmail.com%3e>Discussion</a> in flink mail list.</li><li>Don&rsquo;t support creating iceberg table with computed column.</li><li>Don&rsquo;t support creating iceberg table with watermark.</li><li>Don&rsquo;t support adding columns, removing columns, renaming columns, changing columns. <a href=https://issues.apache.org/jira/browse/FLINK-19062>FLINK-19062</a> is tracking this.</li></ul></div><div id=toc class=markdown-body><div id=full><nav id=TableOfContents><ul><li><a href=#preparation-when-using-flink-sql-client>Preparation when using Flink SQL Client</a></li><li><a href=#flinks-python-api>Flink&rsquo;s Python API</a></li><li><a href=#creating-catalogs-and-using-catalogs>Creating catalogs and using catalogs.</a><ul><li><a href=#catalog-configuration>Catalog Configuration</a></li><li><a href=#hive-catalog>Hive catalog</a></li><li><a href=#hadoop-catalog>Hadoop catalog</a></li><li><a href=#rest-catalog>REST catalog</a></li><li><a href=#custom-catalog>Custom catalog</a></li><li><a href=#create-through-yaml-config>Create through YAML config</a></li><li><a href=#create-through-sql-files>Create through SQL Files</a></li></ul></li><li><a href=#ddl-commands>DDL commands</a><ul><li><a href=#create-database><code>CREATE DATABASE</code></a></li><li><a href=#create-table><code>CREATE TABLE</code></a></li><li><a href=#partitioned-by><code>PARTITIONED BY</code></a></li><li><a href=#create-table-like><code>CREATE TABLE LIKE</code></a></li><li><a href=#alter-table><code>ALTER TABLE</code></a></li><li><a href=#alter-table--rename-to><code>ALTER TABLE .. RENAME TO</code></a></li><li><a href=#drop-table><code>DROP TABLE</code></a></li></ul></li><li><a href=#querying-with-sql>Querying with SQL</a><ul><li><a href=#flink-batch-read>Flink batch read</a></li><li><a href=#flink-streaming-read>Flink streaming read</a></li><li><a href=#flip-27-source-for-sql>FLIP-27 source for SQL</a></li></ul></li><li><a href=#writing-with-sql>Writing with SQL</a><ul><li><a href=#insert-into><code>INSERT INTO</code></a></li><li><a href=#insert-overwrite><code>INSERT OVERWRITE</code></a></li><li><a href=#upsert><code>UPSERT</code></a></li></ul></li><li><a href=#reading-with-datastream>Reading with DataStream</a><ul><li><a href=#batch-read>Batch Read</a></li><li><a href=#streaming-read>Streaming read</a></li></ul></li><li><a href=#reading-with-datastream-flip-27-source>Reading with DataStream (FLIP-27 source)</a><ul><li><a href=#batch-read-1>Batch Read</a></li><li><a href=#streaming-read-1>Streaming read</a></li><li><a href=#read-as-avro-genericrecord>Read as Avro GenericRecord</a></li></ul></li><li><a href=#writing-with-datastream>Writing with DataStream</a><ul><li><a href=#appending-data>Appending data.</a></li><li><a href=#overwrite-data>Overwrite data</a></li><li><a href=#upsert-data>Upsert data</a></li><li><a href=#write-with-avro-genericrecord>Write with Avro GenericRecord</a></li><li><a href=#netrics>Netrics</a></li></ul></li><li><a href=#options>Options</a><ul><li><a href=#read-options>Read options</a></li><li><a href=#write-options>Write options</a></li></ul></li><li><a href=#inspecting-tables>Inspecting tables</a><ul><li><a href=#history>History</a></li><li><a href=#metadata-log-entries>Metadata Log Entries</a></li><li><a href=#snapshots>Snapshots</a></li><li><a href=#files>Files</a></li><li><a href=#manifests>Manifests</a></li><li><a href=#partitions>Partitions</a></li><li><a href=#all-metadata-tables>All Metadata Tables</a></li><li><a href=#references>References</a></li></ul></li><li><a href=#rewrite-files-action>Rewrite files action.</a></li><li><a href=#type-conversion>Type conversion</a><ul><li><a href=#flink-to-iceberg>Flink to Iceberg</a></li><li><a href=#iceberg-to-flink>Iceberg to Flink</a></li></ul></li><li><a href=#future-improvement>Future improvement.</a></li></ul></nav></div></div></div></div></section></body><script src=https://iceberg.apache.org/docs/1.2.0//js/jquery-1.11.0.js></script>
<script src=https://iceberg.apache.org/docs/1.2.0//js/jquery.easing.min.js></script>
<script type=text/javascript src=https://iceberg.apache.org/docs/1.2.0//js/search.js></script>
<script src=https://iceberg.apache.org/docs/1.2.0//js/bootstrap.min.js></script>
<script src=https://iceberg.apache.org/docs/1.2.0//js/iceberg-theme.js></script></html>